<!DOCTYPE html><html domain=.com lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="default-src 'self';object-src 'none';script-src 'self' 'unsafe-inline' https://cdnjs.cloudflare.com/;style-src 'self' https://fonts.googleapis.com/ https://use.fontawesome.com/ 'unsafe-inline';img-src 'self' data:;font-src 'self' https://fonts.gstatic.com/ https://use.fontawesome.com/" http-equiv=Content-Security-Policy><link href=/favicon.svg rel=icon type=image/svg+xml><meta content=#f9c412 name=theme-color><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name=robots><title>TF 4 - Sequences, Time Series and Prediction</title><meta content="TF 4 - Sequences, Time Series and Prediction" property=og:title><meta content="This is my note for the 4th course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..." name=description><meta content="This is my note for the 4th course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..." property=og:description><meta content=summary_large_image name=twitter:card><meta content=@dinhanhthi name=twitter:site><meta content=@dinhanhthi name=twitter:creator><meta content=/img/header/tensorflow.svg property=og:image><link href=https://dinhanhthi.com/deeplearning-ai-tensorflow-course-4/ rel=canonical><meta content=no-referrer-when-downgrade name=referrer><link href=/feed/feed.xml rel=alternate type=application/atom+xml title="ðŸ”¥ Anh-Thi DINH"><link href=/ rel=preconnect crossorigin=""><script src="/js/min.js?hash=70bdd86fac" async defer></script><script csp-hash="">if (/Mac OS X/.test(navigator.userAgent))
      document
        .documentElement
        .classList
        .add('apple')</script><style>@charset "UTF-8";@font-face{font-display:swap;font-family:"Poppins";font-style:normal;font-weight:400;src:url(/fonts/poppins/poppins-v15-latin-regular.eot);src:local("Poppins Regular"),local("Poppins-Regular"),url(/fonts/poppins/poppins-v15-latin-regular.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-regular.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-regular.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-regular.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-regular.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:"Poppins";font-style:italic;font-weight:400;src:url(/fonts/poppins/poppins-v15-latin-italic.eot);src:local("Poppins Italic"),local("Poppins-Italic"),url(/fonts/poppins/poppins-v15-latin-italic.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-italic.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-italic.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-italic.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-italic.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:'Poppins';font-style:normal;font-weight:600;src:url(/fonts/poppins/poppins-v15-latin-600.eot);src:local("Poppins SemiBold"),local("Poppins-SemiBold"),url(/fonts/poppins/poppins-v15-latin-600.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-600.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-600.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-600.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-600.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:'Poppins';font-style:italic;font-weight:600;src:url(/fonts/poppins/poppins-v15-latin-600italic.eot);src:local("Poppins SemiBold Italic"),local("Poppins-SemiBoldItalic"),url(/fonts/poppins/poppins-v15-latin-600italic.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-600italic.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-600italic.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-600italic.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-600italic.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:'Recoleta';src:url(/fonts/recoleta/Recoleta-Bold.woff2) format("woff2"),url(/fonts/recoleta/Recoleta-Bold.woff) format("woff"),url(/fonts/poppins/Recoleta-Bold.ttf) format("truetype");font-weight:700;font-style:bold}*{border:0;box-sizing:border-box}:root{font-size:16.5px}main img{content-visibility:auto}::-webkit-scrollbar{height:10px;width:10px}::-webkit-scrollbar-thumb{background:#3e466b;border-radius:6px}::-webkit-scrollbar-thumb:hover{background:#aaa;cursor:pointer}::-webkit-scrollbar-track{background:#363948}::-webkit-scrollbar-corner{background:#282a36}body,html{font-family:"Poppins",Arial,Helvetica,sans-serif;font-size:16.5px}html{-webkit-text-size-adjust:100%}@supports (font-variation-settings:normal){html{font-family:"Poppins" var alt,Arial,Helvetica,sans-serif}}body{background:#282a36;color:#eee;margin:0;counter-reset:h2counter}strong{font-weight:600;color:#8cc8ff}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}mark{color:inherit;padding:0;background-color:transparent;box-shadow:inset 0 -4px 0 0 #ffd479}a,a:hover{text-decoration:none}a:hover{border-bottom:2px solid #ffd479;color:#fff}a.no-effect:hover{border-bottom:none}img{max-width:100%}main{margin:0 auto}.page-note h3,.page-note>h2,h1,h2,h3{font-family:"Recoleta",Arial,Helvetica,sans-serif}h1{font-size:29.7px}h2{font-size:24.75px}h3{font-size:21.5325px}.page-note h3,.page-note>h2{color:#eee}.page-note h3:hover .direct-link,.page-note>h2:hover .direct-link{display:inline-block}.page-note h3:hover a:hover,.page-note>h2:hover a:hover,a{color:#ffd479}.page-note>h2{font-size:24.75px;margin:37.125px 0 19.8px}.page-note p+ol,.page-note p+ul,.page-note>h2+h3{margin-top:-.5rem}.page-note>h3{font-size:21.5325px;margin-bottom:19.8px}.page-note .direct-link{display:none;color:#777;border-bottom:none;margin-left:3px}.page-note pre+h2,.page-note pre+h3,.page-note>h3{margin-top:37.125px}.page-note h1{counter-reset:h2counter}.page-note h2{counter-reset:h3counter}.page-note h3{counter-reset:h4counter}.page-note h3:before{opacity:.5;content:counter(h2counter) "." counter(h3counter) ".Â Â ";counter-increment:h3counter}.page-note h2:before{content:counter(h2counter) ".Â Â ";counter-increment:h2counter;color:#8cc8ff}.container,header{width:100%;margin:0 auto}.normal{padding:0 16.5px;width:100%}@media (min-width:916.5px){.normal{width:900px}}.mt-2{margin-top:2rem}.page-index .container{padding:2rem 1rem 0}.page-index .main-cats{flex:0 1 calc(100% - 280px);padding-right:1rem}.page-index .main-cats>.category-wrapper{padding-bottom:1.5rem}.page-index .main-cats>.category-wrapper>.category{border:1px solid #3b3e54;border-radius:7px;height:fit-content;background:#2f3240;padding:2rem 1.5rem 1.5rem}.page-index .toc-index{border:1px solid #404040;border-radius:7px;height:fit-content;background:#35373c;flex:0 1 280px;position:sticky;top:60px;padding:1rem 1.1rem}.page-index .toc-index h3{padding-bottom:5px;border-bottom:1px solid #555;margin:0 0 10px;font-size:1.3rem}.page-index .toc-index ul{padding-left:20px;margin:0}.page-index .toc-index p{font-style:italic;color:#999;padding-top:0;margin-bottom:0;font-size:.95rem;margin-top:10px}.page-index .toc-index p a{color:#999;border-bottom:2px solid #999}.page-index .toc-index p a:hover{border-bottom:2px solid #ffd479}@media (max-width:991px){.page-index .main-cats{flex:1 1 100%;order:2;padding-right:0}.page-index .toc-index{flex:1 1 100%;order:1;position:inherit;margin-bottom:1.5rem}.page-index .toc-index ul{column-count:3;-webkit-column-count:3;-moz-column-count:3}}@media (max-width:767px){.page-index .toc-index ul{column-count:2;-webkit-column-count:2;-moz-column-count:2}}@media (max-width:575px){.page-index .toc-index ul{column-count:1;-webkit-column-count:1;-moz-column-count:1}}.category{width:100%}.category h2,header h1{font-size:1.55rem;margin-top:0}.category h2 img{float:left;margin-right:7px}.category .list-homepage{list-style:none;padding-left:10px;margin-bottom:0;column-count:1;-webkit-column-count:1;-moz-column-count:1}@media (min-width:768px){.category .list-homepage{column-count:2;-webkit-column-count:2;-moz-column-count:2}}@media (min-width:992px){.category .list-homepage{column-count:3;-webkit-column-count:3;-moz-column-count:3}}.category .list-homepage li{padding-left:15px;margin-bottom:10px;display:inline-block;width:100%}.category .list-homepage li a{color:#ddd;border-bottom:2px solid rgba(255,255,255,.14);border-style:dotted}.category .list-homepage li::before{content:"ðŸ“„";margin-right:5px;margin-left:-25px;opacity:.8}.category .list-homepage li:hover{cursor:pointer}.category .list-homepage li:hover a{border-color:#ffd479;color:#fff}.category .list-homepage li:hover::before{opacity:1}.page-index header{padding-top:5em;padding-bottom:0}.page-index header .header-logo{height:80px;width:auto}header nav{z-index:10}#nav{z-index:2;position:relative}header{padding:5.5rem 1.5rem 1rem;width:900px;text-align:center;max-width:100%;display:flex;align-items:center;flex-direction:column}header h1{font-size:2.2rem;margin-bottom:0}header p{margin-top:1rem}header .header-logo{width:55px;height:55px;margin-bottom:1rem}header .header-logo img{width:100%;height:100%}header .social{margin-top:.5rem}header #more-info #note-tag>a,header .social a{margin-right:10px}header .social a:last-child{margin-right:0}@media (min-width:992px){header .social a{margin-right:20px}}header .social a img{border-radius:50%}header #more-info{padding:1rem}header #more-info #note-tag{padding-bottom:10px;border-bottom:1px solid #444}header #more-info #note-tag>a::before{content:"#"}header #more-info #last-modified{padding-top:10px;font-style:italic}#reading-progress,nav{top:0;left:0;width:100vw}#reading-progress{z-index:3;border-bottom:1px solid #ffd479;position:absolute;bottom:0;transform:translate(-100vw,0);will-change:transform;pointer-events:none}.intro,.job span{font-size:1.02rem}.intro b,.intro strong{color:#ffd479;font-weight:400}.intro a,footer a:hover{color:#fff}.job span{background:#ffd479;color:#000;padding:3px 10px;border-radius:15px}nav{position:fixed;padding:0 1.5em;background:#35373c}@media (max-width:992px){nav{padding:0 1em}}@media (max-width:576px){nav{padding:0 .5em}}nav #nav{display:-ms-flexbox;-ms-flex-align:center;align-items:center}nav #nav a{color:#ccc;margin-right:15px;align-items:center;padding:0 .5rem;font-size:1.1rem;white-space:nowrap}nav #nav a img{margin-right:5px}nav #nav a:hover{color:#fff;cursor:pointer;text-decoration:none}nav #nav .nav-item{text-align:left;margin-right:5px}@media (min-width:421px){nav #nav .nav-item{padding-left:0!important}}@media (max-width:575px){nav #nav .nav-item{width:unset!important}nav #nav .nav-item span{display:none}}@media (min-width:576px){nav #nav .nav-item{margin-right:15px}}nav #nav .nav-github{text-align:right;padding-right:0!important;margin-right:0!important}nav #nav .nav-search{display:block;background-image:linear-gradient(to right,#3d4251,#3b3f4c,#393d46,#373a41,#35373c);width:100%;position:relative}nav #nav,nav #nav .nav-search form,nav #nav a{display:flex}nav #nav .nav-search .nav-search-input{border:0;background:0 0;color:#ddd;font-size:1.05rem;padding:.65rem .5rem;width:100%}nav #nav .nav-search .nav-search-input:focus{outline:0;border:0}nav #nav .nav-search #search-result{position:absolute;max-height:80vh;overflow:auto;width:100%;background:#35373c;border-bottom-right-radius:5px;border-bottom-left-radius:5px;padding:0 1rem}@media (max-width:767px){nav #nav .nav-search #search-result{position:fixed;left:0;right:0;border-radius:0}}nav #nav .nav-search #search-result ul#searchResults{padding:0 0 0 20px}.page-note ol li,.page-note ul li,div.toc ol li,nav #nav .nav-search #search-result ul#searchResults li h3{margin-bottom:5px}nav #nav .nav-search #search-result ul#searchResults li h3 a{color:#efb232;white-space:inherit;padding:0;text-align:left;font-weight:400;font-size:16.5px;font-family:"Poppins",Arial,Helvetica,sans-serif;line-height:1.4}nav #nav .nav-search #search-result ul#searchResults li h3 a:hover{border-bottom:none;color:#8cc8ff}nav #nav .nav-search #search-result ul#searchResults li p{text-align:left;margin-top:0;line-height:1.4}nav #nav .nav-search #search-result #noResultsFound p{text-align:left}footer{font-size:1.1rem;background:#35373c;padding:.75rem 1rem;text-align:center;margin-top:3rem}footer a{color:#ccc}.columns-2{display:grid;grid-template-columns:1fr 1fr;align-items:center;margin-bottom:24.75px}@media (min-width:576px) and (max-width:767px){.columns-2{grid-template-columns:2fr 1fr}}@media (max-width:575px){.columns-2{grid-template-columns:1fr}}@media (min-width:576px){.columns-2>:nth-child(1){padding-right:5px;margin-bottom:0}.columns-2>:nth-child(1)>:last-child,.columns-2>:nth-child(2)>:last-child{margin-bottom:0}.columns-2>:nth-child(2){padding-left:5px;margin-bottom:0}}.hsbox{margin-bottom:24.75px;border:1px solid #969696;padding:1rem;border-radius:3px}.hsbox .hs__title{cursor:pointer}.hsbox .hs__title::before{content:" ";display:inline-block;border-top:7px solid transparent;border-bottom:7px solid transparent;border-left:7px solid currentColor;vertical-align:middle;margin-right:.7rem;transform:translateY(-2px);transition:transform .2s ease-out}.hsbox .hs__title.show{padding-bottom:15px;border-bottom:.5px solid #666;margin-bottom:1rem}.hsbox .hs__title.show+.hs__content{display:block;opacity:1;padding:5px 0;transition:all .25s 0s cubic-bezier(.4,0,.2,1)}.hsbox .hs__title.show::before{transform:rotate(90deg) translateX(-3px)}.hsbox .hs__content{display:none;transition:all .2s 0s ease}.hsbox .hs__content>:last-child{margin-bottom:0}.katex{font:1.21em KaTeX_Main,Times New Roman,serif;line-height:1.2;text-indent:0;text-rendering:auto}.katex *{-ms-high-contrast-adjust:none!important}.katex .katex-mathml{position:absolute;clip:rect(1px,1px,1px,1px);padding:0;border:0;height:1px;width:1px;overflow:hidden}.katex .base{position:relative;white-space:nowrap;width:min-content}.katex .base,.katex .strut,.katex .vlist>span>span{display:inline-block}.katex .vlist-t{display:inline-table;table-layout:fixed}.katex .vlist-r{display:table-row}.katex .vlist{display:table-cell;vertical-align:bottom;position:relative}.katex .vlist>span,.katex svg{display:block;height:0;position:relative}.katex .vlist>span>.pstrut{overflow:hidden;width:0}.katex .vlist-t2{margin-right:-2px}.katex .vlist-s{display:table-cell;vertical-align:bottom;font-size:1px;width:2px;min-width:2px}.katex .msupsub{text-align:left}.katex .sizing.reset-size6.size3{font-size:.7em}.katex svg{position:absolute;width:100%;height:inherit;fill:currentColor;stroke:currentColor;fill-rule:nonzero;fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1}.katex img{border-style:none;min-width:0;min-height:0;max-width:none;max-height:none}.page-note .text-center{text-align:center}.page-note ol,.page-note p,.page-note ul{margin-top:0;margin-bottom:24.75px}.page-note ol li ol,.page-note ol li ul,.page-note ul li ol,.page-note ul li ul{margin-bottom:5px;padding-left:20px}.page-note ol li>*,.page-note ul li>*{margin-bottom:10px}.page-note p.noindent{display:none;padding-left:20px}.page-note p.noindent+ol,.page-note p.noindent+ul{padding-left:20px}.page-note p.indent{display:none;padding-left:40px}.page-note ol.indent,.page-note p.indent+ol,.page-note p.indent+ul,.page-note ul.indent{padding-left:40px}.page-note ol.noindent,.page-note ul.noindent{padding-left:20px}.page-note hr{border-bottom:1px solid #676767;margin-bottom:24.75px}#reading-list .item .author{font-style:italic}#reading-list .item .intro{color:#999}code[class*=language-],pre[class*=language-]{color:#eee;font-size:16px;text-shadow:none;font-family:Menlo,Monaco,Consolas,"Andale Mono","Ubuntu Mono","Courier New",monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{text-shadow:none;background:#75a7ca}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}.token.comment{color:#6a9955}.token.punctuation{color:#eee}.token.inserted,.token.number{color:#b5cea8}.token.builtin,.token.string{color:#ce9178}.token.operator{color:#eee}.token.keyword{color:#90cdff}.token.function{color:#efefac}.token.boolean,.token.important{color:#90cdff}.token.property{color:#9cdcfe}pre[class*=language-]>code[class*=language-]{position:relative;z-index:1}code,pre{font-family:Consolas,Menlo,Monaco,"Andale Mono WT","Andale Mono","Lucida Console","Lucida Sans Typewriter","DejaVu Sans Mono","Bitstream Vera Sans Mono","Liberation Mono","Nimbus Mono L","Courier New",Courier,monospace;line-height:1.5}h2>code{font-size:21.78px!important}h3>code{font-size:18.9486px!important}:not(pre)>code{border:1px solid #444;background:#6b444245;padding:2px 4px;margin:0 1px;border-radius:3px;font-size:.9rem;color:#fff;word-break:break-word}h2>code,h3>code{color:#ddd;padding-right:6px}a>code{color:#ffd479}a:hover>code{color:#ccc}pre,pre[class*=language-]{margin:0 0 27.225px;overflow:auto}pre>code,pre[class*=language-]>code{display:block;padding:14.5px 16.5px 16.5px;background:#2f3240;border:.5px solid #3b3e54;overflow:auto;border-radius:3px;max-height:450px}div.toc{margin-bottom:24.75px;border:1px solid #3b3e54;border-radius:7px;height:fit-content;background:#2f3240;padding:15px 15px 10px 0}div.toc>ol::before{content:"In this note";display:block;padding-bottom:5px;border-bottom:1px solid #555;margin:0 0 15px;font-size:1.18rem;font-family:"Recoleta",Arial,Helvetica,sans-serif}div.toc ol{padding-left:20px;font-size:14.85px;margin-bottom:0}div.toc ol li code{font-size:.85rem;background:#ececec;padding:0 4px 2px}div.toc ol li ol{padding-left:10px;margin-top:7px}div.toc ol,div.toc ol ol{counter-reset:item;list-style-type:none}div.toc ol li::before{content:counters(item,".") ". ";counter-increment:item}div.toc>ol>li ol>li::before{opacity:.7}div.toc>ol>li::before{color:#8cc8ff}@media (min-width:1300px){div.toc{float:right;margin-right:-280px;border-left:none;width:250px;position:-webkit-sticky;position:sticky;top:60px;max-height:70vh;overflow:auto}div.toc ol{margin-top:0;margin-bottom:0}}@media (min-width:1500px){div.toc{margin-right:-310px;width:280px}}.toc-active>a{color:#fff!important}.toc-active::before{opacity:1!important}.page-note img{height:auto;width:100%}.page-note img.bg-white{background:#fff}@media (min-width:768px){.page-note .img-50,.page-note .img-60{width:50%;margin-left:auto;margin-right:auto;display:block}.page-note .img-60{width:60%}.page-note .img-100,.page-note .img-70,.page-note .img-80{width:70%;margin-left:auto;margin-right:auto;display:block}.page-note .img-100,.page-note .img-80{width:80%}.page-note .img-100{width:100%}}.page-note p>img+br{display:none}.page-note p>img+br+em{display:block;text-align:center;margin-top:10px}</style><header><nav><div id=nav><a href=/ class="nav-item no-effect"><img alt=home src=/img/nav/home.svg height=18 width=18> <span>Thi</span> </a><a href=/about/ class="nav-item no-effect"><img alt=about src=/img/nav/about.svg height=15 width=15> <span>About</span></a><div class=nav-search><form><input aria-label="search notes..." class=nav-search-input id=searchField placeholder="search notes..." type=search></form><div id=search-result style="display: none;"><ul id=searchResults></ul><div id=noResultsFound style="display: none;"><p>No results found.</div></div></div><a href=https://github.com/dinhanhthi class="nav-item no-effect nav-github" target=_blank><img alt=github src=/img/nav/github.svg height=20 width=20></a></div><div id=reading-progress aria-hidden=true></div></nav><div class=header-logo><img alt="TF 4 - Sequences, Time Series and Prediction" src=/img/header/tensorflow.svg></div><h1>TF 4 - Sequences, Time Series and Prediction</h1><div id=more-info><div id=note-tag><a href=/tags/mooc>MOOC</a> <a href=/tags/deeplearning.ai>deeplearning.ai</a> <a href=/tags/deep-learning>Deep Learning</a> <a href=/tags/tensorflow>TensorFlow</a></div><div id=last-modified>03-12-2020 / <a href=https://github.com/dinhanhthi/dinhanhthi.com/edit/dev/./posts/mooc/2020-10-14-deeplearning-ai-tensorflow-course-4.md>Edit on Github</a></div></div></header><main><article><div class="container mt-2 normal page-note"><div class=toc><ol><li><a href=#sequences-and-prediction>Sequences and prediction</a><ol><li><a href=#time-series>Time Series</a><li><a href=#train-%2F-validation-%2F-test>Train / Validation / Test</a><li><a href=#metrics>Metrics</a><li><a href=#moving-average-and-differencing>Moving average and differencing</a></ol><li><a href=#deep-nn-for-time-series>Deep NN for Time Series</a><ol><li><a href=#preparing-features-and-labels>Preparing features and labels</a></ol><li><a href=#sequence-bias>Sequence bias</a><li><a href=#feeding-windowed-datasets-into-nn>Feeding windowed datasets into NN</a><li><a href=#rnn-for-ts>RNN for TS</a><ol><li><a href=#shape-of-input-to-rnn>Shape of input to RNN</a><li><a href=#sequence-to-vector-rnn>Sequence to vector RNN</a><li><a href=#lambda-layer>Lambda layer</a><li><a href=#simple-rnn>Simple RNN</a><li><a href=#lstm>LSTM</a></ol><li><a href=#real-world-time-series-data>Real-world time series data</a></ol></div><p>This is my note for the <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/ >4th course</a> of <a href=https://www.coursera.org/specializations/tensorflow-in-practice>TensorFlow in Practice Specialization</a> given by <a href=http://deeplearning.ai/ >deeplearning.ai</a> and taught by Laurence Moroney on Coursera.<p>ðŸ‘‰ Check the codes <a href=https://github.com/dinhanhthi/deeplearning.ai-courses/tree/master/TensorFlow%20in%20Practice>on my Github</a>.<br>ðŸ‘‰ Official <a href=https://github.com/lmoroney/dlaicourse>notebooks</a> on Github.<p>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-1>course 1 - Intro to TensorFlow for AI, ML, DL</a>.<br>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-2>course 2 - CNN in TensorFlow</a>.<br>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-3>course 3 - NLP in Tensorflow</a>.<p class=noindent><ul><li><strong>Sequence models</strong>: focus on <em>time series</em> (there are others) -- stock, weather,...<li>At the end, we wanna model <strong>sunspot actitivity cycles</strong> which is important to NASA and other space agencies.<li>Using RNN on time series data.</ul><h2 id=sequences-and-prediction>Sequences and prediction <a href=#sequences-and-prediction class=direct-link>#</a></h2><h3 id=time-series>Time Series <a href=#time-series class=direct-link>#</a></h3><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-1/notebook_1_introduction_to_time_series.html>introduction to time series</a>. + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/Pzp5K/introduction-to-time-series>explaining video</a>. => How to create synthetic time series data + plot them.<p class=noindent><ul><li>Time series is everywhere: stock prices, weather focasts, historical trends (Moore's law),...<li><strong>Univariate</strong> TS and <strong>Miltivariate</strong> TS.<li>Type of things can we do with ML over TS:<ul><li>Any thing has a time factor can be analysed using TS.<li>Predicting a focasting (eg. birth & death in Japan -> predict future for retirement, immigration, impacts...).<li><strong>Imputation</strong>: project back into the past.<li>Fill holes in the data.<li>Nomalies detecction (website attacks).<li>Spot patterns (eg. speed recognition).</ul><li>Common patterns in TS:<ul><li><p><strong>Trend</strong>: a specific direcion that they're moving in.<p><img alt=Trend src=/img/post/mooc/tf/trend.png class=img-50><li><p><strong>Seasonality</strong>: patterns repeat at predictable intervals (eg. active users for a website).<p><img alt=seasonality src=/img/post/mooc/tf/seasonality.png class=img-100><li><p>Combinition of both <strong>trend</strong> and <strong>seasonality</strong>.<p><img alt=trend+seasonality src=/img/post/mooc/tf/trend_seasonality.png class=img-50><li><p><strong>Stationary</strong> TS.<p><img alt=stationality src=/img/post/mooc/tf/stationality.png class=img-50><li><p><strong>Autocorrelated</strong> TS: a time series is linearly related to a <em>lagged</em> version of itself.. There is no trend, no seasonality.<p><img alt=autocorrelation src=/img/post/mooc/tf/autocorrelation.png class=img-60><li><p><strong>Multiple auto correlation</strong>.<p><img alt=multiple_autocorrelation src=/img/post/mooc/tf/multiple_autocorrelation.png class=img-60><li><p>May be <strong>trend</strong> + <strong>seasonality</strong> + <strong>autorrelation</strong> + <strong>noise</strong>.<p><img alt=trend_seasonality_autocorrelation_noise src=/img/post/mooc/tf/trend_seasonality_autocorrelation_noise.png class=img-60><li><p><strong>Non-stationary</strong> TS:<p><img alt=non_stationary src=/img/post/mooc/tf/non_stationary.png class=img-60><br><em>In this case, we base just on the later data to predict the future (not on the whole data).</em></ul></ul><h3 id=train-%2F-validation-%2F-test>Train / Validation / Test <a href=#train-%2F-validation-%2F-test class=direct-link>#</a></h3><ul><li><p><strong>Fixed partitioning</strong> (this course focuses on) = splitting TS data into <strong>training period</strong>, <strong>validation period</strong> and <strong>test period</strong>.<ul><li><p>If TS is seasonal, we want each period contains the whole number of seasons.<p><img alt="Fixed partitioning" src=/img/post/mooc/tf/fixed_partitioning.png class=img-70></ul><li><p>We can split + train + test to get a model and then <strong>re-train</strong> with the data <strong>containing also the test period</strong> so that the model is optimized! In that case, the test set comes from the future.<p><img alt="Fixed partitioning with test period comes from the future" src=/img/post/mooc/tf/fixed_partitioning_future_test.png class=img-70><li><p><strong>Roll-forward partitioning</strong>: we start with a short training period and we gradually increase it (1 day at a time or 1 week at a time). At each iteration, we train the model on training period, use it to focast the following day/week in the validation period. = Fixed partitioning in a number of times!<p><img alt="Roll-forward partitioning" src=/img/post/mooc/tf/roll_forward_partitioning.png class=img-70></ul><h3 id=metrics>Metrics <a href=#metrics class=direct-link>#</a></h3><p>For evaluating models:<pre class=language-python><code class=language-python>errors <span class="token operator">=</span> forecasts <span class="token operator">-</span> actual<br><br><span class="token comment"># Mean squared error (square to get rid of negative values)</span><br><span class="token comment"># Eg. Used if large errors are potentially dangerous</span><br>mse <span class="token operator">=</span> np<span class="token punctuation">.</span>square<span class="token punctuation">(</span>errors<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><br><span class="token comment"># Get back to the same scale to error</span><br>rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mse<span class="token punctuation">)</span><br><br><span class="token comment"># Mean absolute error (his favorite)</span><br><span class="token comment"># this doesn't penalize large errs as much as mse does,</span><br><span class="token comment"># used if loss is proportional to the size of err</span><br>mae <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>errors<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br><span class="token comment"># Mean abs percentage err</span><br><span class="token comment"># idea of the size of err compared to the values</span><br>mape <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>errors <span class="token operator">/</span> x_valid<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># MAE with TF</span><br>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>mean_absolute_error<span class="token punctuation">(</span>x_valid<span class="token punctuation">,</span> naive_forecast<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id=moving-average-and-differencing>Moving average and differencing <a href=#moving-average-and-differencing class=direct-link>#</a></h3><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-1/notebook_2_forecasting.html>Forecasting</a>. + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/KVWrR/forecasting>explaining video</a>.<p><strong>Moving average</strong>: a simple forecasting method. Calculate the average of blue lines within a fixed "averaging windows".<ul><li>This can eliminate noises and doesn't anticipate trend or seasonality.<li>Depend on the "averaging window", it can give worse result than naive forecast.</ul><p><img alt="Moving average" src=/img/post/mooc/tf/moving_average.png class=img-70><br><em>Take the average on each yellow window. MAE=7.14 (optimal is 4).</em><pre class=language-python><code class=language-python><span class="token keyword">def</span> <span class="token function">moving_average_forecast</span><span class="token punctuation">(</span>series<span class="token punctuation">,</span> window_size<span class="token punctuation">)</span><span class="token punctuation">:</span><br>    <span class="token string triple-quoted-string">"""Forecasts the mean of the last few values.<br>        If window_size=1, then this is equivalent to naive forecast"""</span><br>    forecast <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br>    <span class="token keyword">for</span> time <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>series<span class="token punctuation">)</span> <span class="token operator">-</span> window_size<span class="token punctuation">)</span><span class="token punctuation">:</span><br>    forecast<span class="token punctuation">.</span>append<span class="token punctuation">(</span>series<span class="token punctuation">[</span>time<span class="token punctuation">:</span>time <span class="token operator">+</span> window_size<span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>forecast<span class="token punctuation">)</span></code></pre><p><strong>Differencing</strong>: remove the trend and seasonality from the TS. We study on the differences between points and their previous neighbor in period.<p><img alt="Moving average on differenced time series" src=/img/post/mooc/tf/moving_avg_on_differenced_ts.jpg class=img-100><br><em>Left image: we find the differencing of original values, then we find the average (orange line). Right image: restore the trend and seasonality. MAE=5.8 (optimal is 4).</em><p>Above method still get the noises (because we add the differencing to the previous noise). If we remove past noise using moving average on that.<p><img alt="Smoothing both past and present values" src=/img/post/mooc/tf/smoothing_both_past_present_values.png class=img-70><br><em>Smoothing both past and present values. MAE=4.5 (optimal is 4).</em><p>Keep in mind before using Deep Learning, <mark>sometimes simple approaches just work fine!</mark><h2 id=deep-nn-for-time-series>Deep NN for Time Series <a href=#deep-nn-for-time-series class=direct-link>#</a></h2><h3 id=preparing-features-and-labels>Preparing features and labels <a href=#preparing-features-and-labels class=direct-link>#</a></h3><ul><li>We need to split our TS data into features and labels so that we can use them in ML algos.<li>In this case: features=#values in TS, label=next_value.<ul><li>Feature: window size and train to predict next value.<li>Ex: 30 days of values as features and next value as label.<li>Overtime, train ML to match 30 features to match a single label.</ul></ul><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-2/notebook_1_preparing_features_and_labels.html>Preparing features and labels</a>.<br>ðŸ‘‰ <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/TYErD/preparing-features-and-labels>Video explains how to split to features and labels from dataset</a>.<pre class=language-python><code class=language-python><span class="token keyword">def</span> <span class="token function">windowed_dataset</span><span class="token punctuation">(</span>series<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle_buffer<span class="token punctuation">)</span><span class="token punctuation">:</span><br>    dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_tensor_slices<span class="token punctuation">(</span>series<span class="token punctuation">)</span><br>    dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>window<span class="token punctuation">(</span>window_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> shift<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> drop_remainder<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br>    dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>flat_map<span class="token punctuation">(</span><span class="token keyword">lambda</span> window<span class="token punctuation">:</span> window<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>window_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>shuffle_buffer<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> window<span class="token punctuation">:</span> <span class="token punctuation">(</span>window<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> window<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">.</span>prefetch<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><br>    <span class="token keyword">return</span> dataset</code></pre><div class=hsbox><div class=hs__title>Explaine the codes</div><div class=hs__content><pre class=language-python><code class=language-python><span class="token comment"># create a very simple dataset</span><br>dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><br>arr <span class="token operator">=</span> <span class="token punctuation">[</span>val<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> val <span class="token keyword">in</span> dataset<span class="token punctuation">]</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><br><span class="token comment"># [0, 1, 2, 3, 4, 5]</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># make equal (drop_remaninder) windows</span><br>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>window<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> shift<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> drop_remainder<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>flat_map<span class="token punctuation">(</span><span class="token keyword">lambda</span> window<span class="token punctuation">:</span> window<span class="token punctuation">.</span>batch<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    <span class="token comment"># instead of val.numpy for each val in each window</span><br><span class="token keyword">for</span> window <span class="token keyword">in</span> dataset<span class="token punctuation">:</span><br>    <span class="token keyword">print</span><span class="token punctuation">(</span>window<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token comment"># [0 1 2 3 4]</span><br><span class="token comment"># [1 2 3 4 5]</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># split the last value to be label</span><br>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> window<span class="token punctuation">:</span> <span class="token punctuation">(</span>window<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> window<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token comment"># [0 1 2 3] [4]</span><br><span class="token comment"># [1 2 3 4] [5]</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># shuffle</span><br>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>buffer_size<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span><br><span class="token comment"># construct batch of 2</span><br>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>prefetch<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><br><span class="token comment"># x =  [[1 2 3 4], [0 1 2 3]]</span><br><span class="token comment"># y =  [[5], [4]]</span></code></pre></div></div><h2 id=sequence-bias>Sequence bias <a href=#sequence-bias class=direct-link>#</a></h2><p>Sequence bias is when the order of things can impact the selection of things. <mark>It's ok to shuffle!</mark><h2 id=feeding-windowed-datasets-into-nn>Feeding windowed datasets into NN <a href=#feeding-windowed-datasets-into-nn class=direct-link>#</a></h2><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-2/notebook_2_1layer_NN_linear_reg.html>Single layer NN</a> + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/YERBd/more-on-single-layer-neural-network>video explains it</a>.<pre class=language-python><code class=language-python><span class="token comment"># Simple linear regression (1 layer NN)</span><br>dataset <span class="token operator">=</span> windowed_dataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle_buffer_size<span class="token punctuation">)</span><br>l0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">[</span>window_size<span class="token punctuation">]</span><span class="token punctuation">)</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>l0<span class="token punctuation">]</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">"mse"</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><br><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Layer weights {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>l0<span class="token punctuation">.</span>get_weights<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><br>forecast <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br><br><span class="token keyword">for</span> time <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>series<span class="token punctuation">)</span> <span class="token operator">-</span> window_size<span class="token punctuation">)</span><span class="token punctuation">:</span><br>    forecast<span class="token punctuation">.</span>append<span class="token punctuation">(</span>model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>series<span class="token punctuation">[</span>time<span class="token punctuation">:</span>time <span class="token operator">+</span> window_size<span class="token punctuation">]</span><span class="token punctuation">[</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    <span class="token comment"># np.newaxis: reshape X to input dimension that used by the model</span><br><br>forecast <span class="token operator">=</span> forecast<span class="token punctuation">[</span>split_time<span class="token operator">-</span>window_size<span class="token punctuation">:</span><span class="token punctuation">]</span><br>results <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>forecast<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span></code></pre><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-2/notebook_3_DNN_TS.html>DNN with TS</a> + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/HecKT/deep-neural-network-training-tuning-and-prediction>video explains it</a>.<pre class=language-python><code class=language-python><span class="token comment"># A way to choose an optimal learning rate</span><br>lr_schedule <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>LearningRateScheduler<span class="token punctuation">(</span><br>    <span class="token keyword">lambda</span> epoch<span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">8</span> <span class="token operator">*</span> <span class="token number">10</span><span class="token operator">**</span><span class="token punctuation">(</span>epoch <span class="token operator">/</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">8</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">"mse"</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">)</span><br>history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> callbacks<span class="token operator">=</span><span class="token punctuation">[</span>lr_schedule<span class="token punctuation">]</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></code></pre><div class=columns-2><pre class=language-python><code class=language-python>lrs <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">8</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">10</span> <span class="token operator">**</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>plt<span class="token punctuation">.</span>semilogx<span class="token punctuation">(</span>lrs<span class="token punctuation">,</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><img alt="Loss w.r.t different learning rates." src=/img/post/mooc/tf/c4_w2_lr.png class="img-100 bg-white"><br><em>Loss w.r.t different learning rates. We choose the lowest one, around 8e-6.</em></div><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-2/notebook_4_DNN_synthetic_data.html>DNN with synthetic TS</a>.<h2 id=rnn-for-ts>RNN for TS <a href=#rnn-for-ts class=direct-link>#</a></h2><p class=noindent><ul><li>RRN is a NN containing Recurrent layer.<li>The different from DNN is the input shape is <strong>3 dimensional</strong> (<code>batch_size x #time_step x dims_input_at each_timestep</code>).<li>Re-use 1 cell multiple times in different layers (in this course).</ul><p><img alt="Idea of how RNN works with TS data." src=/img/post/mooc/tf/rnn_ts_idea.png class=img-100><br><em>Idea of how RNN works with TS data. The current location can be impacted more by the nearby locations.</em><h3 id=shape-of-input-to-rnn>Shape of input to RNN <a href=#shape-of-input-to-rnn class=direct-link>#</a></h3><p>ðŸ‘‰ <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/fP3ND/shape-of-the-inputs-to-the-rnn>Video explains the dimensional and sequence-to-vector RNN</a>.<p class=noindent><ul><li>Suppose: <em>window size</em> of 30 time steps, <em>batch size</em> of 4: Shape will be 4x30x1 and the <em>memory cell</em> input will be 4x1 matrix.<li>If the memory cell comprises 3 neurons then the <em>output matrix</em> will be 4x3. Therefore, the full output of the layer will be 4x30x3.<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>H</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>H_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.83333em;vertical-align:-0.15em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.31166399999999994em;><span style=top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>â€‹</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span></span> is just a copy of <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>Y_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.83333em;vertical-align:-0.15em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.22222em;>Y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.31166399999999994em;><span style=top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>â€‹</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span></span>.<li>Below figure: input and also output a sequence.</ul><p><img alt="Dimension of input to RNN." src=/img/post/mooc/tf/rnn_ts_dim.png class=img-100><br><em>Dimension of input to RNN.</em><h3 id=sequence-to-vector-rnn>Sequence to vector RNN <a href=#sequence-to-vector-rnn class=direct-link>#</a></h3><p class=noindent><ul><li>Sometimes, we want only input a sequence but not output. This called <strong>sequence-to-vector RNN</strong>. I.E., <mark>ignore all of the outputs except the last one!</mark>. In <code>tf.keras</code>, it's default setting!</ul><p><img alt="Sequence to vector RNN." src=/img/post/mooc/tf/rnn_ts_sequence_to_vector.png class=img-100><br><em>Sequence to vector RNN.</em><pre class=language-python><code class=language-python><span class="token comment"># Check the figure below as an illustration</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SimpleRNN<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment"># input_shape:</span><br>    <span class="token comment">#   TF assumes that 1st dim is batch size -> any size at all -> no need to define</span><br>    <span class="token comment">#   None -> number of time steps, None means RNN can handle sequence of any length</span><br>    <span class="token comment">#   1 -> univariate TS</span><br>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SimpleRNN<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment"># if there is `return_sequences=True` -> sequence-to-sequence RNN</span><br>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><img alt="Illustration with keras." src=/img/post/mooc/tf/rnn_ts_illustraction_with_keras.png class=img-80><br><em>Illustration with keras.</em><h3 id=lambda-layer>Lambda layer <a href=#lambda-layer class=direct-link>#</a></h3><p>ðŸ‘‰ <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/I0K6b/lambda-layers>Video explains the use of lambda layer in RNN.</a>.<pre class=language-python><code class=language-python>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> tf<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># expand to 1 dim (from 2) so that we have 3 dims: batch size x #timesteps x series dim</span><br>                        input_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># can use any size of sequences</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SimpleRNN<span class="token punctuation">(</span><span class="token number">40</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SimpleRNN<span class="token punctuation">(</span><span class="token number">40</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">*</span> <span class="token number">100.0</span><span class="token punctuation">)</span><br>        <span class="token comment"># default activation in RNN is tanh -> (-1, 1) -> scale to -100, 100</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id=simple-rnn>Simple RNN <a href=#simple-rnn class=direct-link>#</a></h3><p class=noindent><ul><li>Loss function <strong>Huber</strong> (<a href=https://en.wikipedia.org/wiki/Huber_loss>wiki</a>): less sensitive to outliers. => we use this because our data in this case get a little bit noisy!</ul><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-3/notebook_1_simple_RNN_with_TS.html>Simple RNN with a TS data</a> + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/5W1Rw/rnn>videos explains it</a>.<h3 id=lstm>LSTM <a href=#lstm class=direct-link>#</a></h3><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-3/notebook_2_LSTM_with_TS.html>LSTM with a TS data</a> + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/IqcpX/more-on-lstm>videos explains it</a>.<pre class=language-python><code class=language-python><span class="token comment"># clear internal variables</span><br>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span><br>dataset <span class="token operator">=</span> windowed_dataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle_buffer_size<span class="token punctuation">)</span><br><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> tf<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>                        input_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment"># LSTM here</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">*</span> <span class="token number">100.0</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-3/notebook_3_LSTM_synthetic_data.html>LSTM with synthetic TS</a>.<h2 id=real-world-time-series-data>Real-world time series data <a href=#real-world-time-series-data class=direct-link>#</a></h2><p class=noindent><ul><li>We are going to predict the <strong>sunspot actitivity cycles</strong> (<a href=https://www.kaggle.com/robervalt/sunspots>download dataset</a>).<li>Combine CNN + LSTM.</ul><p>ðŸ‘‰ Andrew's <a href="https://www.youtube.com/watch?v=4qJaSmvhxi8">video on Optimization Algo: Mini-batch gradient descent</a>.<br>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-4/notebook_1_sunspot_cnn_lstm.html>Sunspot dataset with CNN+LSTM</a>. + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/zAaeD/combining-our-tools-for-analysis>video explains it</a>.<br>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-4/notebook_2_sunspot_DNN_only.html>Sunspot dataset with DNN only</a> + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/EPaeW/sunspots>explaining video</a>.<br>ðŸ‘‰ <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/LYbcx/train-and-tune-the-model>Video explains train & tune the model</a> (how to choose suitable values for sizes)</div><script type=application/ld+json>{
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": "TF 4 - Sequences, Time Series and Prediction",
        "image": [],
        "author": "Anh-Thi DINH",
        "genre": "Insert a schema.org genre",
        "url": "https://dinhanhthi.com/deeplearning-ai-tensorflow-course-4/",
        "mainEntityOfPage": "https://dinhanhthi.com/deeplearning-ai-tensorflow-course-4/",
        "datePublished": "13-10-2020",
        "dateModified": "03-12-2020",
        "description": "This is my note for the 4th course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..."
      }</script></article></main><footer><a href=/ target=_blank>Thi Â Â©Â  2020 </a>Â â€¢Â  <a href=/about-the-notes/ >About the notes </a>Â â€¢Â  <a href=https://pobo.dinhanhthi.com target=_blank>Po Bo </a>Â â€¢Â  <a href=/for-me-only/ >For me only </a>Â â€¢Â  <a href=/donate/ >Support Thi</a></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/elasticlunr/0.9.6/elasticlunr.min.js></script>