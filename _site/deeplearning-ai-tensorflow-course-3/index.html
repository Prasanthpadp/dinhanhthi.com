<!DOCTYPE html><html domain=".com" lang="en"><head><meta charset="utf-8"><meta content="width=device-width,initial-scale=1" name="viewport"><meta content="default-src 'self';object-src 'none';script-src 'self' 'unsafe-inline' https://cdnjs.cloudflare.com/ https://gist.github.com/;style-src 'self' https://fonts.googleapis.com/ https://use.fontawesome.com/ 'unsafe-inline' https://github.githubassets.com/;img-src 'self' data:;font-src 'self' https://fonts.gstatic.com/ https://use.fontawesome.com/" http-equiv="Content-Security-Policy"><link href="/favicon.svg" rel="icon" type="image/svg+xml"><meta content="#f9c412" name="theme-color"><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name="robots"><title>TF 3 - NLP in TensorFlow</title><meta content="TF 3 - NLP in TensorFlow" property="og:title"><meta content="This is my note for the 3rd course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..." name="description"><meta content="This is my note for the 3rd course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..." property="og:description"><meta content="summary_large_image" name="twitter:card"><meta content="@dinhanhthi" name="twitter:site"><meta content="@dinhanhthi" name="twitter:creator"><meta content="https://dinhanhthi.com/img/remote/Z1qeDpw.svg" property="og:image"><meta content="article" property="og:type"><link href="https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/" rel="canonical"><meta content="https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/" property="og:url"><meta content="no-referrer-when-downgrade" name="referrer"><link href="/feed/feed.xml" rel="alternate" type="application/atom+xml" title="🔥 Anh-Thi DINH"><link href="/" rel="preconnect" crossorigin=""><script src="https://cdnjs.cloudflare.com/ajax/libs/elasticlunr/0.9.6/elasticlunr.min.js"></script><script src="/js/min.js?hash=46f4d2fefc" async="" defer=""></script><script csp-hash="sha256-ZO6+QmLYhAEQAnjl/xe643cTrnHA858+XwtOmaWwuN8=">if (/Mac OS X/.test(navigator.userAgent))
      document
        .documentElement
        .classList
        .add('apple')</script><style>@charset "UTF-8";@font-face{font-display:swap;font-family:"Poppins";font-style:normal;font-weight:400;src:url(/fonts/poppins/poppins-v15-latin-regular.eot);src:local("Poppins Regular"),local("Poppins-Regular"),url(/fonts/poppins/poppins-v15-latin-regular.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-regular.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-regular.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-regular.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-regular.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:"Poppins";font-style:italic;font-weight:400;src:url(/fonts/poppins/poppins-v15-latin-italic.eot);src:local("Poppins Italic"),local("Poppins-Italic"),url(/fonts/poppins/poppins-v15-latin-italic.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-italic.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-italic.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-italic.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-italic.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:'Poppins';font-style:normal;font-weight:600;src:url(/fonts/poppins/poppins-v15-latin-600.eot);src:local("Poppins SemiBold"),local("Poppins-SemiBold"),url(/fonts/poppins/poppins-v15-latin-600.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-600.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-600.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-600.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-600.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:'Poppins';font-style:italic;font-weight:600;src:url(/fonts/poppins/poppins-v15-latin-600italic.eot);src:local("Poppins SemiBold Italic"),local("Poppins-SemiBoldItalic"),url(/fonts/poppins/poppins-v15-latin-600italic.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-600italic.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-600italic.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-600italic.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-600italic.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:'Recoleta';src:url(/fonts/recoleta/Recoleta-Bold.woff2) format("woff2"),url(/fonts/recoleta/Recoleta-Bold.woff) format("woff"),url(/fonts/poppins/Recoleta-Bold.ttf) format("truetype");font-weight:700;font-style:bold}*{border:0;box-sizing:border-box}:root{font-size:16.5px}main img{content-visibility:auto}::-webkit-scrollbar{height:10px;width:10px}::-webkit-scrollbar-thumb{background:#3e466b;border-radius:6px}::-webkit-scrollbar-thumb:hover{background:#aaa;cursor:pointer}::-webkit-scrollbar-track{background:#363948}::-webkit-scrollbar-corner{background:#282a36}body,html{font-family:"Poppins",Arial,Helvetica,sans-serif;font-size:16.5px}html{-webkit-text-size-adjust:100%}@supports (font-variation-settings:normal){html{font-family:"Poppins" var alt,Arial,Helvetica,sans-serif}}body{background:#282a36;color:#eee;margin:0;counter-reset:h2counter}b,strong{font-weight:600;color:#8cc8ff}small{font-size:80%}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}mark{color:inherit;padding:0;background-color:transparent;box-shadow:inset 0 -4px 0 0 #ffd479}a,a:hover{text-decoration:none}a:hover{border-bottom:2px solid #ffd479}a.no-effect:hover{border-bottom:none}.page-note h3:hover a:hover,.page-note>h2:hover a:hover,a,a b,a strong{color:#ffd479}.intro a,a b:hover,a strong:hover,a:hover{color:#fff}img{max-width:100%}main{margin:0 auto}.page-note h3,.page-note>h2,h1,h2,h3{font-family:"Recoleta",Arial,Helvetica,sans-serif}h1{font-size:29.7px}h2{font-size:24.75px}h3{font-size:21.5325px}.page-note h3,.page-note>h2{color:#eee}.page-note h3:hover .direct-link,.page-note>h2:hover .direct-link{display:inline-block}.page-note>h2{font-size:24.75px;margin:37.125px 0 19.8px}.page-note p+ol,.page-note p+ul,.page-note>h2+h3{margin-top:-.5rem}.page-note>h3{font-size:21.5325px;margin-bottom:19.8px}.page-note .direct-link{display:none;color:#777;border-bottom:none;margin-left:3px}.page-note pre+h2,.page-note pre+h3,.page-note>h3{margin-top:37.125px}.page-note h1{counter-reset:h2counter}.page-note h2{counter-reset:h3counter}.page-note h3{counter-reset:h4counter}.page-note h3:before{opacity:.5;content:counter(h2counter) "." counter(h3counter) ".  ";counter-increment:h3counter}.page-note h2:before{content:counter(h2counter) ".  ";counter-increment:h2counter;color:#8cc8ff}.container,header{width:100%;margin:0 auto}.normal{padding:0 16.5px;width:100%}@media (min-width:916.5px){.normal{width:900px}}.mt-2{margin-top:2rem}.page-index .container{padding:2rem 1rem 0}.page-index .main-cats{flex:0 1 calc(100% - 280px);padding-right:1rem}.page-index .main-cats>.category-wrapper{padding-bottom:1.5rem}.page-index .main-cats>.category-wrapper>.category{border:1px solid #3b3e54;border-radius:7px;height:fit-content;background:#2f3240;padding:2rem 1.5rem 1.5rem}.page-index .toc-index{border:1px solid #404040;border-radius:7px;height:fit-content;background:#35373c;flex:0 1 280px;position:sticky;top:60px;padding:1rem 1.1rem}.page-index .toc-index h3{padding-bottom:5px;border-bottom:1px solid #555;margin:0 0 10px;font-size:1.3rem}.page-index .toc-index ul{padding-left:20px;margin:0}.page-index .toc-index p{font-style:italic;color:#999;padding-top:0;margin-bottom:0;font-size:.95rem;margin-top:10px}.page-index .toc-index p a{color:#999;border-bottom:2px solid #999}.page-index .toc-index p a:hover{border-bottom:2px solid #ffd479}@media (max-width:991px){.page-index .main-cats{flex:1 1 100%;order:2;padding-right:0}.page-index .toc-index{flex:1 1 100%;order:1;position:inherit;margin-bottom:1.5rem}.page-index .toc-index ul{column-count:3;-webkit-column-count:3;-moz-column-count:3}}@media (max-width:767px){.page-index .toc-index ul{column-count:2;-webkit-column-count:2;-moz-column-count:2}}@media (max-width:575px){.page-index .toc-index ul{column-count:1;-webkit-column-count:1;-moz-column-count:1}}.category{width:100%}.category h2,header h1{font-size:1.55rem;margin-top:0}.category h2 img{float:left;margin-right:7px}.category .list-homepage{list-style:none;padding-left:10px;margin-bottom:0;column-count:1;-webkit-column-count:1;-moz-column-count:1}@media (min-width:768px){.category .list-homepage{column-count:2;-webkit-column-count:2;-moz-column-count:2}}@media (min-width:992px){.category .list-homepage{column-count:3;-webkit-column-count:3;-moz-column-count:3}}.category .list-homepage li{padding-left:15px;margin-bottom:10px;display:inline-block;width:100%}.category .list-homepage li a{color:#ddd;border-bottom:2px solid rgba(255,255,255,.14);border-style:dotted}.category .list-homepage li::before{content:"📄";margin-right:5px;margin-left:-25px;opacity:.8}.category .list-homepage li:hover{cursor:pointer}.category .list-homepage li:hover a{border-color:#ffd479;color:#fff}.category .list-homepage li:hover::before{opacity:1}.page-index header{padding-top:5em;padding-bottom:0}.page-index header .header-logo{height:80px;width:auto}header nav{z-index:10}#nav{z-index:2;position:relative}header{padding:5.5rem 1.5rem 1rem;width:900px;text-align:center;max-width:100%;display:flex;align-items:center;flex-direction:column}header h1{font-size:2.2rem;margin-bottom:0}header p{margin-top:1rem}header .header-logo{width:55px;height:55px;margin-bottom:1rem}header .header-logo img{width:100%;height:100%}header .social{margin-top:.5rem}header #more-info #note-tag>a,header .social a{margin-right:10px}header .social a:last-child{margin-right:0}@media (min-width:992px){header .social a{margin-right:20px}}header .social a img{border-radius:50%}header #more-info{padding:1rem}header #more-info #note-tag{padding-bottom:10px;border-bottom:1px solid #444}header #more-info #note-tag>a::before{content:"#"}header #more-info #last-modified{padding-top:10px;font-style:italic}#reading-progress,nav{top:0;left:0;width:100vw}#reading-progress{z-index:3;border-bottom:1px solid #ffd479;position:absolute;bottom:0;transform:translate(-100vw,0);will-change:transform;pointer-events:none}.intro,.job span{font-size:1.02rem}.intro b,.intro strong{color:#ffd479;font-weight:400}.job span{background:#ffd479;color:#000;padding:3px 10px;border-radius:15px}nav{position:fixed;padding:0 1.5em;background:#35373c}@media (max-width:992px){nav{padding:0 1em}}@media (max-width:576px){nav{padding:0 .5em}}nav #nav{display:-ms-flexbox;-ms-flex-align:center;align-items:center}nav #nav a{color:#ccc;margin-right:15px;align-items:center;padding:0 .5rem;font-size:1.1rem;white-space:nowrap}nav #nav a img{margin-right:5px}nav #nav a:hover{color:#fff;cursor:pointer;text-decoration:none}nav #nav .nav-item{text-align:left;margin-right:5px}@media (min-width:421px){nav #nav .nav-item{padding-left:0!important}}@media (max-width:575px){nav #nav .nav-item{width:unset!important}nav #nav .nav-item span{display:none}}@media (min-width:576px){nav #nav .nav-item{margin-right:15px}}nav #nav .nav-github{text-align:right;padding-right:0!important;margin-right:0!important}nav #nav .nav-github:hover img{filter:invert(.4)}nav #nav .nav-search{display:block;background-image:linear-gradient(to right,#3d4251,#3b3f4c,#393d46,#373a41,#35373c);width:100%;position:relative}nav #nav,nav #nav .nav-search form,nav #nav a{display:flex}nav #nav .nav-search .nav-search__input{border:0;background:0 0;color:#ddd;font-size:1.05rem;padding:.65rem .5rem;width:100%}nav #nav .nav-search .nav-search__input:focus{outline:0;border:0}nav #nav .nav-search #nav-search__result-container{position:absolute;max-height:80vh;overflow:auto;width:100%;background:#35373c;border-bottom-right-radius:5px;border-bottom-left-radius:5px;padding:0;filter:drop-shadow(2px 4px 6px #000)}@media (max-width:767px){nav #nav .nav-search #nav-search__result-container{position:fixed;left:0;right:0;border-radius:0}}nav #nav .nav-search #nav-search__result-container ul#nav-search__ul{padding:0;margin:0;list-style:none}nav #nav .nav-search #nav-search__result-container ul#nav-search__ul li{padding:10px 10px 10px 25px}nav #nav .nav-search #nav-search__result-container ul#nav-search__ul li h3{margin:0}nav #nav .nav-search #nav-search__result-container ul#nav-search__ul li h3 a{color:#efb232;white-space:inherit;padding:0;text-align:left;font-weight:400;font-size:16.5px;font-family:"Poppins",Arial,Helvetica,sans-serif;line-height:1.4}nav #nav .nav-search #nav-search__result-container ul#nav-search__ul li h3 a:hover{border-bottom:none;color:#8cc8ff}nav #nav .nav-search #nav-search__result-container ul#nav-search__ul li p{text-align:left;margin:0;line-height:1.4}nav #nav .nav-search #nav-search__result-container ul#nav-search__ul li.selected{background:#282a36}nav #nav .nav-search #nav-search__result-container ul#nav-search__ul li.selected::before{content:"⏎";float:left;color:#8cc8ff;margin-left:-20px}nav #nav .nav-search #nav-search__result-container #nav-search__no-result{padding:0 10px}nav #nav .nav-search #nav-search__result-container #nav-search__no-result p{text-align:left}footer{font-size:1.05rem;background:#35373c;padding:.75rem 1rem;text-align:center;margin-top:3rem}footer a{color:#ccc}footer a:hover{color:#fff}.info{position:relative;padding:16.5px;margin-bottom:24.75px;border:1px solid transparent;border-radius:.25rem;color:#eee;border-left-width:15px;border-color:#8cc8ff}.info :last-child{margin-bottom:0}.hsbox{margin-bottom:24.75px;border:1px solid #969696;padding:1rem;border-radius:3px}.hsbox .hs__title{cursor:pointer}.hsbox .hs__title::before{content:" ";display:inline-block;border-top:7px solid transparent;border-bottom:7px solid transparent;border-left:7px solid currentColor;vertical-align:middle;margin-right:.7rem;transform:translateY(-2px);transition:transform .2s ease-out}.hsbox .hs__title.show{padding-bottom:15px;border-bottom:.5px solid #666;margin-bottom:1rem}.hsbox .hs__title.show+.hs__content{display:block;opacity:1;padding:5px 0;transition:all .25s 0s cubic-bezier(.4,0,.2,1)}.hsbox .hs__title.show::before{transform:rotate(90deg) translateX(-3px)}.hsbox .hs__content{display:none;transition:all .2s 0s ease}.hsbox .hs__content>:last-child{margin-bottom:0}.page-note .text-center{text-align:center}.page-note ol,.page-note p,.page-note ul{margin-top:0;margin-bottom:24.75px}.page-note ol li,.page-note ul li,div.toc ol li{margin-bottom:5px}.page-note ol li ol,.page-note ol li ul,.page-note ul li ol,.page-note ul li ul{margin-bottom:5px;padding-left:20px}.page-note ol li>*,.page-note ul li>*{margin-bottom:10px}.page-note p.noindent{display:none;padding-left:20px}.page-note p.noindent+ol,.page-note p.noindent+ul{padding-left:20px}.page-note p.indent{display:none;padding-left:40px}.page-note ol.indent,.page-note p.indent+ol,.page-note p.indent+ul,.page-note ul.indent{padding-left:40px}.page-note ol.noindent,.page-note ul.noindent{padding-left:20px}.page-note hr{border-bottom:1px solid #676767;margin-bottom:24.75px}#reading-list .item .author{font-style:italic}#reading-list .item .intro{color:#999}code[class*=language-],pre[class*=language-]{color:#eee;font-size:16px;text-shadow:none;font-family:Menlo,Monaco,Consolas,"Andale Mono","Ubuntu Mono","Courier New",monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{text-shadow:none;background:#75a7ca}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}.token.comment{color:#6a9955}.token.punctuation{color:#eee}.token.inserted,.token.number{color:#b5cea8}.token.builtin,.token.string{color:#ce9178}.token.operator{color:#eee}.token.keyword{color:#90cdff}.token.function{color:#efefac}.token.boolean,.token.important{color:#90cdff}.token.property{color:#9cdcfe}pre[class*=language-]>code[class*=language-]{position:relative;z-index:1}code,pre{font-family:Consolas,Menlo,Monaco,"Andale Mono WT","Andale Mono","Lucida Console","Lucida Sans Typewriter","DejaVu Sans Mono","Bitstream Vera Sans Mono","Liberation Mono","Nimbus Mono L","Courier New",Courier,monospace;line-height:1.5}h2>code{font-size:21.78px!important}h3>code{font-size:18.9486px!important}:not(pre)>code{border:1px solid #444;background:#6b444245;padding:2px 4px;margin:0 1px;border-radius:3px;font-size:.9rem;color:#fff;word-break:break-word}h2>code,h3>code{color:#ddd;padding-right:6px}a>code{color:#ffd479}a:hover>code{color:#ccc}pre,pre[class*=language-]{margin:0 0 27.225px;overflow:auto}pre>code,pre[class*=language-]>code{display:block;padding:14.5px 16.5px 16.5px;background:#2f3240;border:.5px solid #3b3e54;overflow:auto;border-radius:3px;max-height:450px}div.toc{margin-bottom:24.75px;border:1px solid #3b3e54;border-radius:7px;height:fit-content;background:#2f3240;padding:15px 15px 10px 0}div.toc>ol::before{content:"In this note";display:block;padding-bottom:5px;border-bottom:1px solid #555;margin:0 0 15px;font-size:1.18rem;font-family:"Recoleta",Arial,Helvetica,sans-serif}div.toc ol{padding-left:20px;font-size:14.85px;margin-bottom:0}div.toc ol li code{font-size:.85rem;background:#ececec;padding:0 4px 2px}div.toc ol li ol{padding-left:10px;margin-top:7px}div.toc ol,div.toc ol ol{counter-reset:item;list-style-type:none}div.toc ol li::before{content:counters(item,".") ". ";counter-increment:item}div.toc>ol>li ol>li::before{opacity:.7}div.toc>ol>li::before{color:#8cc8ff}@media (min-width:1300px){div.toc{float:right;margin-right:-280px;border-left:none;width:250px;position:-webkit-sticky;position:sticky;top:60px;max-height:70vh;overflow:auto}div.toc ol{margin-top:0;margin-bottom:0}}@media (min-width:1500px){div.toc{margin-right:-310px;width:280px}}.toc-active>a{color:#fff!important}.toc-active::before{opacity:1!important}.page-note img{height:auto;width:100%}@media (min-width:768px){.page-note .img-70,.page-note .img-75,.page-note .img-90{width:70%;margin-left:auto;margin-right:auto;display:block}.page-note .img-75,.page-note .img-90{width:75%}.page-note .img-90{width:90%}}.page-note p>img+br,.page-note p>picture+br{display:none}.page-note p>img+br+em,.page-note p>picture+br+em{display:block;text-align:center;margin-top:10px}</style></head><body><header><nav><div id="nav"><a href="/" class="nav-item no-effect"><img alt="home" height="18" src="/img/nav/home.svg" width="18"> <span>Thi</span> </a><a href="/about/" class="nav-item no-effect"><img alt="about" height="15" src="/img/nav/about.svg" width="15"> <span>About</span></a><div id="nav-search" class="nav-search"><form><input aria-label="search notes (type &quot;/&quot; to focus)..." class="nav-search__input" id="nav-search__input" onfocusin="inFocus(this)" placeholder="search notes (type &quot;/&quot; to focus)..." type="search"></form><div id="nav-search__result-container" style="display: none;"><ul id="nav-search__ul"></ul><div id="nav-search__no-result" style="display: none;"><p>No results found.</p></div></div></div><a href="https://github.com/dinhanhthi" class="nav-item no-effect nav-github" target="_blank"><img alt="github" height="20" src="/img/nav/github.svg" width="20"></a></div><div id="reading-progress" aria-hidden="true"></div></nav><script src="/js/search.js?hash=f83afd9451" async="" defer=""></script><div class="header-logo"><img alt="TF 3 - NLP in TensorFlow" height="123" src="/img/header/tensorflow.svg" width="115"></div><h1>TF 3 - NLP in TensorFlow</h1><div id="more-info"><div id="note-tag"><a href="/tags/mooc">MOOC</a> <a href="/tags/deeplearning.ai">deeplearning.ai</a> <a href="/tags/deep-learning">Deep Learning</a> <a href="/tags/tensorflow">TensorFlow</a></div><div id="last-modified">03-12-2020 / <a href="https://github.com/dinhanhthi/dinhanhthi.com/edit/dev/./posts/mooc/2020-09-14-deeplearning-ai-tensorflow-course-3.md">Edit on Github</a></div></div></header><main><article><div class="container mt-2 normal page-note"><div class="toc"><ol><li><a href="#tokernizing-%2B-padding">Tokernizing + padding</a></li><li><a href="#word-embeddings">Word embeddings</a><ol><li><a href="#imdb-review-dataset">IMDB review dataset</a></li><li><a href="#sarcasm-dataset">Sarcasm dataset</a></li></ol></li><li><a href="#pre-tokenized-datasets">Pre-tokenized datasets</a></li><li><a href="#sequence-models">Sequence models</a><ol><li><a href="#rnn-idea">RNN idea</a></li><li><a href="#lstm-idea">LSTM idea</a></li><li><a href="#with-vs-without-lstm">With vs without LSTM</a></li><li><a href="#using-a-convnet">Using a ConvNet</a></li><li><a href="#imdb-dataset">IMDB dataset</a></li></ol></li><li><a href="#sequence-models-and-literature">Sequence models and literature</a></li></ol></div><p>This is my note for the <a href="https://www.coursera.org/learn/natural-language-processing-tensorflow">3rd course</a> of <a href="https://www.coursera.org/specializations/tensorflow-in-practice">TensorFlow in Practice Specialization</a> given by <a href="http://deeplearning.ai/">deeplearning.ai</a> and taught by Laurence Moroney on Coursera.</p><p>👉 Check the codes <a href="https://github.com/dinhanhthi/deeplearning.ai-courses/tree/master/TensorFlow%20in%20Practice">on my Github</a>.<br>👉 Official <a href="https://github.com/lmoroney/dlaicourse">notebooks</a> on Github.</p><p>👉 Go to <a href="/deeplearning-ai-tensorflow-course-1">course 1 - Intro to TensorFlow for AI, ML, DL</a>.<br>👉 Go to <a href="/deeplearning-ai-tensorflow-course-2">course 2 - CNN in TensorFlow</a>.<br>👉 Go to <a href="/deeplearning-ai-tensorflow-course-4">course 4 - Sequences, Time Series and Prediction</a>.</p><h2 id="tokernizing-%2B-padding">Tokernizing + padding <a href="#tokernizing-%2B-padding" class="direct-link">#</a></h2><p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-1/notebook_1_tokenizer_basic_examples.html">Tokenizer basic examples.</a><br>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-1/notebook_2_sarcasm_detection.html">Sarcasm detection</a>.</p><p class="noindent"></p><ul><li>A common simple character encoding is ASCII,</li><li>We can encode each word as a number (token) -- <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"><code>Tokenizer</code></a>.</li><li>Tokenize words &gt; build all the words to make a corpus &gt; turn your sentences into lists of values based on these tokens. &gt; manipulate these lists (make the same length, for example)</li></ul><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer<br><br>sentences <span class="token operator">=</span> <span class="token punctuation">[</span><br>    <span class="token string">'i love my dog'</span><span class="token punctuation">,</span><br>    <span class="token string">'I, love my cat'</span><span class="token punctuation">,</span><br>    <span class="token string">'You love my dog so much!'</span><br><span class="token punctuation">]</span><br><br>tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>num_words <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> oov_token<span class="token operator">=</span><span class="token string">"&lt;OOV&gt;"</span><span class="token punctuation">)</span><br>            <span class="token comment"># num_words: max of words to be tokenized &amp; pick</span><br>            <span class="token comment">#   the most common 100 words.</span><br>            <span class="token comment"># More words, more accuracy, more time to train</span><br>            <span class="token comment"># oov_token: replace unseen words by "&lt;OOV&gt;"</span><br>tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span> <span class="token comment"># fix texts based on tokens</span></code></pre><pre class="language-python"><code class="language-python"><span class="token comment"># indexing words</span><br>word_index <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>word_index<br><span class="token keyword">print</span><span class="token punctuation">(</span>word_index<span class="token punctuation">)</span><br><span class="token comment"># {'&lt;OOV&gt;': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7, 'so': 8, 'much': 9}</span><br><span class="token comment"># "!", ",", capital, ... are removed</span></code></pre><p>👉 <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer">tf.keras.preprocessing.text.Tokenizer</a></p><pre class="language-python"><code class="language-python"><span class="token comment"># encode sentences</span><br>sequences <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>sequences<span class="token punctuation">)</span><br><span class="token comment"># [[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5, 8, 9]]</span><br><span class="token comment"># if a word is not in the word index, it will be lost in the text_to_sequences()</span></code></pre><p>👉 <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences">tf.keras.preprocessing.sequence.pad_sequences</a></p><pre class="language-python"><code class="language-python"><span class="token comment"># make encoded sentences equal</span><br><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>sequence <span class="token keyword">import</span> pad_sequences<br><br>padded <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><br>                       maxlen<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"post"</span><span class="token punctuation">,</span> truncating<span class="token operator">=</span><span class="token string">"post"</span><span class="token punctuation">)</span><br>         <span class="token comment"># maxlen: max len of encoded sentence</span><br>         <span class="token comment"># value: value to be filld (default 0)</span><br>         <span class="token comment"># padding: add missing values at beginning or ending of sentence?</span><br>         <span class="token comment"># truncating: longer than maxlen? cut at beginning or ending?</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>padded<span class="token punctuation">)</span><br><span class="token comment"># [[ 4  2  3  5 -1]</span><br><span class="token comment">#  [ 4  2  3  6 -1]</span><br><span class="token comment">#  [ 7  2  3  5  8]]</span></code></pre><p>👉 <a href="https://rishabhmisra.github.io/publications/">Sarcasm detection dataset.</a></p><pre class="language-python"><code class="language-python"><span class="token comment"># read json text</span><br><span class="token keyword">import</span> json<br><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"/tmp/sarcasm.json"</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span><br>    datastore <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span><br><br>sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br>labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br>urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br><span class="token keyword">for</span> item <span class="token keyword">in</span> datastore<span class="token punctuation">:</span><br>    sentences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'headline'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>    labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'is_sarcastic'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>    urls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'article_link'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h2 id="word-embeddings">Word embeddings <a href="#word-embeddings" class="direct-link">#</a></h2><p>👉 <a href="https://projector.tensorflow.org/">Embedding projector - visualization of high-dimensional data</a><br>👉 <a href="http://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Dataset</a></p><h3 id="imdb-review-dataset">IMDB review dataset <a href="#imdb-review-dataset" class="direct-link">#</a></h3><p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_1_IMDB_reviews.html">Train IMDB review dataset</a>.<br>👉 <a href="https://www.coursera.org/lecture/natural-language-processing-tensorflow/notebook-for-lesson-1-Q1Ln5">Video explain the code</a>.</p><p class="noindent"></p><ul><li><strong>Word embeddings</strong> = the idea in which words and associated words are <em>clustered as vectors</em> in a multi-dimensional space. That allows words with similar meaning to have a similar representation.</li><li>The meaning of the words can come from labeling of the dataset.<ul><li><em>Example</em>: "dull" and "boring" show up a lot in negative reviews =&gt; they have similar sentiments =&gt; they are close to each other in the sentence =&gt; thus their vector will be similar =&gt; NN train + learn these vectors + associating them with the labels to come up with what's called in embedding.</li></ul></li><li>The purpose of <em>embedding dimension</em> is the number of dimensions for the vector representing the word encoding.</li></ul><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<br><span class="token keyword">print</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span> <span class="token comment"># check version of tensorflow</span><br><br><span class="token comment"># If you are using tf1, you need below code</span><br>tf<span class="token punctuation">.</span>enable_eager_execution<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class="language-python"><code class="language-python"><span class="token comment"># IMDB reviews dataset</span><br><span class="token keyword">import</span> tensorflow_datasets <span class="token keyword">as</span> tfds<br>imdb<span class="token punctuation">,</span> info <span class="token operator">=</span> tfds<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"imdb_reviews"</span><span class="token punctuation">,</span> with_info<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> as_supervised<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br><br>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> imdb<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> imdb<span class="token punctuation">[</span><span class="token string">'test'</span><span class="token punctuation">]</span><br><br><span class="token keyword">for</span> s<span class="token punctuation">,</span>l <span class="token keyword">in</span> train_data<span class="token punctuation">:</span> <span class="token comment"># "s" for sentences "l" for labels</span><br>    <span class="token comment"># The values for "s" and "l" are tensors</span><br>    <span class="token comment"># so we need to extracr their values</span><br>    training_sentences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf8'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    training_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>l<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class="language-python"><code class="language-python"><span class="token comment"># Prepare for the NN</span><br>vocab_size <span class="token operator">=</span> <span class="token number">10000</span><br>embedding_dim <span class="token operator">=</span> <span class="token number">16</span> <span class="token comment"># embedding to dim 16</span><br>max_length <span class="token operator">=</span> <span class="token number">120</span> <span class="token comment"># of each sentence</span><br>trunc_type<span class="token operator">=</span><span class="token string">'post'</span> <span class="token comment"># cut the last words</span><br>oov_tok <span class="token operator">=</span> <span class="token string">"&lt;OOV&gt;"</span> <span class="token comment"># replace not-encoded words by this</span><br><br><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer<br><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>sequence <span class="token keyword">import</span> pad_sequences<br><br>tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>num_words <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> oov_token<span class="token operator">=</span>oov_tok<span class="token punctuation">)</span><br>tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>training_sentences<span class="token punctuation">)</span><br>    <span class="token comment"># encoding the words</span><br>word_index <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>word_index<br>    <span class="token comment"># list of word index (built based on training set)</span><br>    <span class="token comment"># there may be many oov_tok in test set</span><br>sequences <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>training_sentences<span class="token punctuation">)</span><br>    <span class="token comment"># apply on sentences</span><br>padded <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span>maxlen<span class="token operator">=</span>max_length<span class="token punctuation">,</span> truncating<span class="token operator">=</span>trunc_type<span class="token punctuation">)</span><br>    <span class="token comment"># padding the sentences</span><br><br><span class="token comment"># apply to the test set</span><br>testing_sequences <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>testing_sentences<span class="token punctuation">)</span><br>testing_padded <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>testing_sequences<span class="token punctuation">,</span>maxlen<span class="token operator">=</span>max_length<span class="token punctuation">)</span></code></pre><pre class="language-python"><code class="language-python"><span class="token comment"># Simple NN</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> input_length<span class="token operator">=</span>max_length<span class="token punctuation">)</span><span class="token punctuation">,</span><br>                              <span class="token comment"># The result of embedding will be a 2D array:</span><br>                              <span class="token comment"># length of sentence x embedding_dim</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment"># Alternatively (a little diff on speed and accuracy):</span><br>    <span class="token comment"># tf.keras.layers.GlobalAveragePooling1D()</span><br>    <span class="token comment">#   average across the vectors to flatten it out</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'binary_crossentropy'</span><span class="token punctuation">,</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class="language-python"><code class="language-python"><span class="token comment"># Training</span><br>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>padded<span class="token punctuation">,</span> training_labels_final<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>testing_padded<span class="token punctuation">,</span> testing_labels_final<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class="language-python"><code class="language-python"><span class="token comment"># the result</span><br>e <span class="token operator">=</span> model<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token comment"># get the result of the embedding layers</span><br>weights <span class="token operator">=</span> e<span class="token punctuation">.</span>get_weights<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>weights<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># shape: (vocab_size, embedding_dim)</span></code></pre><p>If you wanna visualize the result (in 3D) with <a href="https://projector.tensorflow.org/">Embedding projector</a>,</p><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> io<br><br>out_v <span class="token operator">=</span> io<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'vecs.tsv'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><br>out_m <span class="token operator">=</span> io<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'meta.tsv'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><br><span class="token keyword">for</span> word_num <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">:</span><br>  word <span class="token operator">=</span> reverse_word_index<span class="token punctuation">[</span>word_num<span class="token punctuation">]</span><br>  embeddings <span class="token operator">=</span> weights<span class="token punctuation">[</span>word_num<span class="token punctuation">]</span><br>  out_m<span class="token punctuation">.</span>write<span class="token punctuation">(</span>word <span class="token operator">+</span> <span class="token string">"\n"</span><span class="token punctuation">)</span><br>  out_v<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> embeddings<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n"</span><span class="token punctuation">)</span><br>out_v<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><br>out_m<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br><span class="token keyword">try</span><span class="token punctuation">:</span><br>  <span class="token keyword">from</span> google<span class="token punctuation">.</span>colab <span class="token keyword">import</span> files<br><span class="token keyword">except</span> ImportError<span class="token punctuation">:</span><br>  <span class="token keyword">pass</span><br><span class="token keyword">else</span><span class="token punctuation">:</span><br>  files<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string">'vecs.tsv'</span><span class="token punctuation">)</span><br>  files<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string">'meta.tsv'</span><span class="token punctuation">)</span></code></pre><h3 id="sarcasm-dataset">Sarcasm dataset <a href="#sarcasm-dataset" class="direct-link">#</a></h3><p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_2_sacarsm.html">Train Sacarsm dataset</a>.</p><ul><li>In text data, it usually happens that the accuracy increase over the number of training but the loss increase sharply also. We can "play" with hyperparameter to see the effect.</li></ul><pre class="language-python"><code class="language-python"><span class="token comment"># Run this to ensure TensorFlow 2.x is used</span><br><span class="token keyword">try</span><span class="token punctuation">:</span><br>  <span class="token comment"># %tensorflow_version only exists in Colab.</span><br>  <span class="token operator">%</span>tensorflow_version <span class="token number">2.</span>x<br><span class="token keyword">except</span> Exception<span class="token punctuation">:</span><br>  <span class="token keyword">pass</span></code></pre><h2 id="pre-tokenized-datasets">Pre-tokenized datasets <a href="#pre-tokenized-datasets" class="direct-link">#</a></h2><p>👉 <a href="https://github.com/tensorflow/datasets/blob/master/docs/catalog/imdb_reviews.md">datasets/imdb_reviews.md at master · tensorflow/datasets</a><br>👉 <a href="https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder">tfds.features.text.SubwordTextEncoder &nbsp;|&nbsp; TensorFlow Datasets</a><br>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_3_pre-tokenizer.html">Pre-tokenizer example</a>.<br>👉 <a href="https://www.coursera.org/lecture/natural-language-processing-tensorflow/notebook-for-lesson-3-piQXt">Video exaplain the codes</a>.</p><ul><li>There are someones who did the work (tokenization) for you.</li><li>Try on IMDB dataset that has been pre-tokenized.</li><li>The tokenization is done on <strong>subwords</strong>!</li><li>The sequence of words can be just important as their existence.</li></ul><pre class="language-python"><code class="language-python"><span class="token comment"># load imdb dataset from tensorflow</span><br><span class="token keyword">import</span> tensorflow_datasets <span class="token keyword">as</span> tfds<br>imdb<span class="token punctuation">,</span> info <span class="token operator">=</span> tfds<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"imdb_reviews/subwords8k"</span><span class="token punctuation">,</span> with_info<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> as_supervised<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br><br><span class="token comment"># extract train/test sets</span><br>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> imdb<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> imdb<span class="token punctuation">[</span><span class="token string">'test'</span><span class="token punctuation">]</span><br><br><span class="token comment"># take the tokernizer</span><br>tokenizer <span class="token operator">=</span> info<span class="token punctuation">.</span>features<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>encoder<br><br><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>subwords<span class="token punctuation">)</span><br><span class="token comment"># ['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_',...</span></code></pre><pre class="language-python"><code class="language-python">sample_string <span class="token operator">=</span> <span class="token string">'TensorFlow, from basics to mastery'</span><br><br>tokenized_string <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>sample_string<span class="token punctuation">)</span><br><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'Tokenized string is {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>tokenized_string<span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token comment"># Tokenized string is [6307, 2327, 4043, 2120, 2, 48, 4249, 4429, 7, 2652, 8050]</span><br><br>original_string <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>tokenized_string<span class="token punctuation">)</span><br><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'The original string: {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>original_string<span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token comment"># The original string: TensorFlow, from basics to mastery</span></code></pre><pre class="language-python"><code class="language-python"><span class="token comment"># take a look on tokenized string</span><br><span class="token comment"># case sensitive + punctuation maintained</span><br><span class="token keyword">for</span> ts <span class="token keyword">in</span> tokenized_string<span class="token punctuation">:</span><br>  <span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'{} ----&gt; {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>ts<span class="token punctuation">,</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span>ts<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><br><span class="token comment"># 6307 ----&gt; Ten</span><br><span class="token comment"># 2327 ----&gt; sor</span><br><span class="token comment"># 4043 ----&gt; Fl</span><br><span class="token comment"># ...</span></code></pre><ul><li>The code run quite long (4 minutes each epoch if using GPU on colab) because there are a lot of hyperparameters and sub-words.</li><li>Result: 50% acc &amp; loss is decreasing but very small.<ul><li>Because we are using sub-words, not for-words -&gt; they (sub-words) are nonsensical. -&gt; they are only when we put them together in sequences -&gt; <strong>learning from sequences would be a great way forward</strong> -&gt; <strong>RNN</strong> (Recurrent Neural Networks)</li></ul></li></ul><h2 id="sequence-models">Sequence models <a href="#sequence-models" class="direct-link">#</a></h2><ul><li>The relative ordering, the sequence of words, matters for the meaning of the sentence .</li><li>For NN to take into account for the <strong>ordering of the words</strong>: <strong>RNN</strong> (Recurrent Neural Networks), <strong>LSTM</strong> (Long short-term memory).</li><li><strong>Why not RNN but LSTM ?</strong> With RNN, the context is preserved from timstamp to timestamp BUT that may get lost in longer sentences =&gt; LSTM gets better because it has cell state.</li><li><strong>Example of using LSTM</strong>: "<em>I grew up in Ireland, I went to school and at school, they made me learn how to speak...</em>" =&gt; "speak" is the context and we go back to the beginning to catch "Ireland", then the next word could be "leanr how to speak <strong>Gaelic</strong>"!</li></ul><h3 id="rnn-idea">RNN idea <a href="#rnn-idea" class="direct-link">#</a></h3><p>👉 <a href="/deeplearning-ai-course-5">Note of the course of sequence model</a>.</p><p class="noindent"></p><ul><li>The usual NN, something like "f(data, labels)=rules" cannot take into account of sequences.</li><li><strong>An example of using sequences</strong>: Fibonacci sequence =&gt; the result of current function is the input of next function itself,...</li></ul><p><picture><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/post/mooc/tf/rnn-basic-idea-1920w.webp 1920w, /img/post/mooc/tf/rnn-basic-idea-1280w.webp 1280w, /img/post/mooc/tf/rnn-basic-idea-640w.webp 640w, /img/post/mooc/tf/rnn-basic-idea-320w.webp 320w" type="image/webp"><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/post/mooc/tf/rnn-basic-idea-1920w.jpg 1920w, /img/post/mooc/tf/rnn-basic-idea-1280w.jpg 1280w, /img/post/mooc/tf/rnn-basic-idea-640w.jpg 640w, /img/post/mooc/tf/rnn-basic-idea-320w.jpg 320w" type="image/jpeg"><img alt="RNN basic idea" height="226" src="/img/post/mooc/tf/rnn-basic-idea.png" width="861" class="pop img-70" decoding="async" loading="lazy" style="background-size:cover;contain-intrinsic-size: min(var(--main-width), 861px) min(calc(var(--main-width) * 0.26248548199767713), 226px);background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http%3A//www.w3.org/2000/svg' xmlns%3Axlink='http%3A//www.w3.org/1999/xlink' viewBox='0 0 861 226'%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='.5'%3E%3C/feGaussianBlur%3E%3CfeComponentTransfer%3E%3CfeFuncA type='discrete' tableValues='1 1'%3E%3C/feFuncA%3E%3C/feComponentTransfer%3E%3C/filter%3E%3Cimage filter='url(%23b)' preserveAspectRatio='none' height='100%25' width='100%25' xlink%3Ahref='data%3Aimage/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAECAIAAADec/LeAAAACXBIWXMAAAsSAAALEgHS3X78AAAAnElEQVQI1zWOWQ6DMAxEuf/hUFkkSGlKQISyQwiQkLVWpfrLb+TxTOCcQ2F5C+V/o5UpHgQWHNVGW5I1+3wAwgF+VEE/9GkRNm0NNmedVOLd5vxkuMnYsZa0WPnonefHVtI82DlDVbxsk3NeKS3V9ayTaRlyEo1Lh0g6rK245Dh3oAeQEhdU3P8mxgLCsxR/jLGo6hZ2gi6VTl70CzI7p1NKh8hpAAAAAElFTkSuQmCC'%3E%3C/image%3E%3C/svg%3E&quot;)"></picture><br><em>RNN basic idea (<a href="https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359">source</a>).</em></p><h3 id="lstm-idea">LSTM idea <a href="#lstm-idea" class="direct-link">#</a></h3><p>👉 (Video) <a href="https://www.youtube.com/watch?v=8HyCNIVRbSU&amp;feature=emb_title">Illustrated Guide to LSTM's and GRU's: A step by step explanation</a> &amp; <a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">its article</a>.</p><p class="noindent"></p><ul><li>Sometimes, the sequence context leads to lose information like the example of "Ireland" and "Gaelic" before.</li><li>LSTM has an additional pipeline called <strong>Cell State</strong>. It can pass through the network to impact it + help to keep context from earlier tokens relevance.</li></ul><p><picture><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/post/mooc/tf/lstm-basic-idea-1920w.webp 1920w, /img/post/mooc/tf/lstm-basic-idea-1280w.webp 1280w, /img/post/mooc/tf/lstm-basic-idea-640w.webp 640w, /img/post/mooc/tf/lstm-basic-idea-320w.webp 320w" type="image/webp"><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/post/mooc/tf/lstm-basic-idea-1920w.jpg 1920w, /img/post/mooc/tf/lstm-basic-idea-1280w.jpg 1280w, /img/post/mooc/tf/lstm-basic-idea-640w.jpg 640w, /img/post/mooc/tf/lstm-basic-idea-320w.jpg 320w" type="image/jpeg"><img alt="LSTM basic idea" height="333" src="/img/post/mooc/tf/lstm-basic-idea.png" width="796" class="pop img-75" decoding="async" loading="lazy" style="background-size:cover;contain-intrinsic-size: min(var(--main-width), 796px) min(calc(var(--main-width) * 0.4183417085427136), 333px);background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http%3A//www.w3.org/2000/svg' xmlns%3Axlink='http%3A//www.w3.org/1999/xlink' viewBox='0 0 796 333'%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='.5'%3E%3C/feGaussianBlur%3E%3CfeComponentTransfer%3E%3CfeFuncA type='discrete' tableValues='1 1'%3E%3C/feFuncA%3E%3C/feComponentTransfer%3E%3C/filter%3E%3Cimage filter='url(%23b)' preserveAspectRatio='none' height='100%25' width='100%25' xlink%3Ahref='data%3Aimage/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAwAAAAFCAYAAABxeg0vAAAACXBIWXMAAAsSAAALEgHS3X78AAAAxklEQVQI1z2OsU7CUABF37/yAU4O7O46msDEZGLCwiCbwIYQEkNLoDbYB5SCpbRU2ldIn8dqA3e4Obl3OYIix3PIIQu4xE9cNOd/zvKEIN1cPxGHCik9HOkS+inB9hvLtvHcgGinWK+2zD+dYj+ikhNibTsY9Spm7RbH+MDqvDB6qDB+ukdaHkbjDvPxhlmvQxxrBHnGwWyzf2+BVuTRgq/BM6l8KwR+iKev+MNm4V1qib+aeTBeXjXpTiBSJbsh9O2Sda75BSip3TaWwkW2AAAAAElFTkSuQmCC'%3E%3C/image%3E%3C/svg%3E&quot;)"></picture><br><em>LSTM basic idea (image from the course).</em></p><pre class="language-python"><code class="language-python"><span class="token comment"># SINGLE LAYER LSTM</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>      <span class="token comment"># 64: #oututs desired (but the result may be different)</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_1_IMDB_subwords_8K_with_single_layer_LSTM.html">IMDB Subwords 8K with Single Layer LSTM</a></p><pre class="language-python"><code class="language-python"><span class="token comment"># MULTI PLAYER LSTM</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>      <span class="token comment"># return_sequences=True: required if we wanna feed LSTM into another one</span><br>      <span class="token comment"># It ensures that the output of LSTM match the desired inputs of the next one</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_2_IMDB_subwords_8K_with_multi_layer_LSTM.html">IMDB Subwords 8K with Multi Layer LSTM</a></p><p><picture><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/post/mooc/tf/1layer-vs-2layer-lstm-1920w.webp 1920w, /img/post/mooc/tf/1layer-vs-2layer-lstm-1280w.webp 1280w, /img/post/mooc/tf/1layer-vs-2layer-lstm-640w.webp 640w, /img/post/mooc/tf/1layer-vs-2layer-lstm-320w.webp 320w" type="image/webp"><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/post/mooc/tf/1layer-vs-2layer-lstm-1920w.jpg 1920w, /img/post/mooc/tf/1layer-vs-2layer-lstm-1280w.jpg 1280w, /img/post/mooc/tf/1layer-vs-2layer-lstm-640w.jpg 640w, /img/post/mooc/tf/1layer-vs-2layer-lstm-320w.jpg 320w" type="image/jpeg"><img alt="1layer vs 2 later LSTM acc" height="812" src="/img/post/mooc/tf/1layer-vs-2layer-lstm.png" width="1730" class="pop img-90" decoding="async" loading="lazy" style="background-size:cover;contain-intrinsic-size: min(var(--main-width), 1730px) min(calc(var(--main-width) * 0.46936416184971097), 812px);background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http%3A//www.w3.org/2000/svg' xmlns%3Axlink='http%3A//www.w3.org/1999/xlink' viewBox='0 0 1730 812'%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='.5'%3E%3C/feGaussianBlur%3E%3CfeComponentTransfer%3E%3CfeFuncA type='discrete' tableValues='1 1'%3E%3C/feFuncA%3E%3C/feComponentTransfer%3E%3C/filter%3E%3Cimage filter='url(%23b)' preserveAspectRatio='none' height='100%25' width='100%25' xlink%3Ahref='data%3Aimage/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAFCAYAAACTphZWAAAACXBIWXMAAAsSAAALEgHS3X78AAAAh0lEQVQI1y2OWwrDMAwEff+jtlDS2NbTzlQJ/VgWRitp25SFaCKS9JmoLbiAfT26/mJD25m4B+oVLPfyvTYZ9xHDzNByd6f1Pjg+L0J7DWYF4wme3wM3YWc8ygiaTGEc7wJWwOjnSUTV6kctrqoAKsock3b3WbkY00v1tl7eNW6m6vQuD3NzfqzA1x3w+9QyAAAAAElFTkSuQmCC'%3E%3C/image%3E%3C/svg%3E&quot;)"></picture><br><em>1 layer vs 2 layer LSTM accuracy after 50 epochs (image from the course). 2 layer is better (smoother) which makes us more confident about the model. The validation acc is sticked to 80% because we used 8000 sub-words taken from training set, so there may be many tokens from the test set that would be out of vocabulary.</em></p><h3 id="with-vs-without-lstm">With vs without LSTM <a href="#with-vs-without-lstm" class="direct-link">#</a></h3><pre class="language-python"><code class="language-python"><span class="token comment"># WITHOUT LSTM (like previous section)</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span><br>                              input_length<span class="token operator">=</span>max_length<span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GlobalmaxPooling1D<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><pre class="language-python"><code class="language-python"><span class="token comment"># WITH LSTM</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span><br>                              input_length<span class="token operator">=</span>max_length<span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><picture><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/post/mooc/tf/with-vs-without-lstm-1920w.webp 1920w, /img/post/mooc/tf/with-vs-without-lstm-1280w.webp 1280w, /img/post/mooc/tf/with-vs-without-lstm-640w.webp 640w, /img/post/mooc/tf/with-vs-without-lstm-320w.webp 320w" type="image/webp"><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/post/mooc/tf/with-vs-without-lstm-1920w.jpg 1920w, /img/post/mooc/tf/with-vs-without-lstm-1280w.jpg 1280w, /img/post/mooc/tf/with-vs-without-lstm-640w.jpg 640w, /img/post/mooc/tf/with-vs-without-lstm-320w.jpg 320w" type="image/jpeg"><img alt="With vs without LSTM" height="354" src="/img/post/mooc/tf/with-vs-without-lstm.png" width="864" class="pop img-90" decoding="async" loading="lazy" style="background-size:cover;contain-intrinsic-size: min(var(--main-width), 864px) min(calc(var(--main-width) * 0.4097222222222222), 354px);background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http%3A//www.w3.org/2000/svg' xmlns%3Axlink='http%3A//www.w3.org/1999/xlink' viewBox='0 0 864 354'%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='.5'%3E%3C/feGaussianBlur%3E%3CfeComponentTransfer%3E%3CfeFuncA type='discrete' tableValues='1 1'%3E%3C/feFuncA%3E%3C/feComponentTransfer%3E%3C/filter%3E%3Cimage filter='url(%23b)' preserveAspectRatio='none' height='100%25' width='100%25' xlink%3Ahref='data%3Aimage/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAwAAAAFCAYAAABxeg0vAAAACXBIWXMAAAsSAAALEgHS3X78AAAAiElEQVQI1z2PAQ4EIQgD/f9jN1kVFRC7lUvOZIJibbHoUogIxiCsIh3bHbED9a1Yc8HMMVgXtaW3hikN4UYhMUuxTs0H5kE2ToD9g1LfF06HXAe53xTc5Od5aKCIi2ualkUnU0uuqDHx1hOH4w24KuYYTJxYHLtc17zsktzzH/wSe+O/OOZh7wNAser3D5iRVgAAAABJRU5ErkJggg=='%3E%3C/image%3E%3C/svg%3E&quot;)"></picture><br><em>With vs without LSTM (image from the course). With LSTM is really better but there is still overfitting here.</em></p><h3 id="using-a-convnet">Using a ConvNet <a href="#using-a-convnet" class="direct-link">#</a></h3><p>👉 <a href="https://www.coursera.org/lecture/natural-language-processing-tensorflow/using-a-convolutional-network-fSE8o">Video explains the dimension</a>.<br>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_3_IMDB_subwords_8K_with_Conv.html">IMDB Subwords 8K with 1D Convolutional Layer</a>.</p><pre class="language-python"><code class="language-python">model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv1D<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GlobalAveragePooling1D<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><picture><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/post/mooc/tf/using-conv-net-1920w.webp 1920w, /img/post/mooc/tf/using-conv-net-1280w.webp 1280w, /img/post/mooc/tf/using-conv-net-640w.webp 640w, /img/post/mooc/tf/using-conv-net-320w.webp 320w" type="image/webp"><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/post/mooc/tf/using-conv-net-1920w.jpg 1920w, /img/post/mooc/tf/using-conv-net-1280w.jpg 1280w, /img/post/mooc/tf/using-conv-net-640w.jpg 640w, /img/post/mooc/tf/using-conv-net-320w.jpg 320w" type="image/jpeg"><img alt="Using Convolution network." height="247" src="/img/post/mooc/tf/using-conv-net.png" width="825" class="pop img-90" decoding="async" loading="lazy" style="background-size:cover;contain-intrinsic-size: min(var(--main-width), 825px) min(calc(var(--main-width) * 0.2993939393939394), 247px);background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http%3A//www.w3.org/2000/svg' xmlns%3Axlink='http%3A//www.w3.org/1999/xlink' viewBox='0 0 825 247'%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='.5'%3E%3C/feGaussianBlur%3E%3CfeComponentTransfer%3E%3CfeFuncA type='discrete' tableValues='1 1'%3E%3C/feFuncA%3E%3C/feComponentTransfer%3E%3C/filter%3E%3Cimage filter='url(%23b)' preserveAspectRatio='none' height='100%25' width='100%25' xlink%3Ahref='data%3Aimage/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAECAYAAAC+0w63AAAACXBIWXMAAAsSAAALEgHS3X78AAAAf0lEQVQI102PWQ4DIQxDuf9Z+SgJDjuewFRVkZ5QZDtLQAaLVWYUwg71MvrgXpsxRqaP3LoZWJFYTRlusA1y8/cMdo29dR6da3E2cI3uvtcYRJSAujh9wrpUe4NzTKqINzCX19v8SyhnPfhUCE2T/8qSz2rdzZviwZyN+gf8rAdkTdqlgieQzgAAAABJRU5ErkJggg=='%3E%3C/image%3E%3C/svg%3E&quot;)"></picture><br><em>Using Convolution network. (image from the course). It's really better but there is overfitting there.</em></p><h3 id="imdb-dataset">IMDB dataset <a href="#imdb-dataset" class="direct-link">#</a></h3><p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_4_IMDB_review_with_GRU.html">IMDB Reviews with GRU (and optional LSTM and Conv1D)</a>.<br>👉 <a href="https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/NFvFd/going-back-to-the-imdb-dataset">Video compares the results</a>.</p><p>Try with 3 different choices:</p><p class="indent"></p><ul><li><strong>Simple NN</strong>: 5s/epoch, 170K params, nice acc, overfitting.</li><li><strong>LSTM</strong>: 43s/epoch, 30K params, acc better, overfitting.</li><li><strong>GRU</strong> (Gated Recurrent Unit layer, a different type of RNN): 20s/epoch, 169K params, very good acc, overfitting.</li><li><strong>Conv1D</strong>: 6s/epoch, 171K params, good acc, overfitting.</li></ul><p><strong>Remark</strong>: <mark>With the texts, you'll probably get a bit more overfitting than you would have done with images.</mark> Because we have out of voca words in validation data.</p><h2 id="sequence-models-and-literature">Sequence models and literature <a href="#sequence-models-and-literature" class="direct-link">#</a></h2><p>One application of sequence models: read text then <strong>generate another look-alike text</strong>.</p><p>👉 <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_1_find_the_next_word_trained_from_a_song.html">Notebook 1</a> &amp; <a href="https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/B80b0/notebook-for-lesson-1">explaining video</a>.</p><ul><li>How they predict a new word in the notebook? -&gt; Check <a href="https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/LGBS2/predicting-a-word">this video</a>.</li></ul><pre class="language-python"><code class="language-python">input_sequences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br><span class="token keyword">for</span> line <span class="token keyword">in</span> corpus<span class="token punctuation">:</span><br>	<span class="token comment"># convert each sentence to list of numbers</span><br>	token_list <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span>line<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><br>	<span class="token comment"># convert each list to n-gram sequence</span><br>	<span class="token comment"># eg. from [1,2,3,4,5]</span><br>	<span class="token comment"># 		to [1,2], [1,2,3], [1,2,3,4], [1,2,3,4,5]</span><br>	<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>token_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><br>		n_gram_sequence <span class="token operator">=</span> token_list<span class="token punctuation">[</span><span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><br>		input_sequences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>n_gram_sequence<span class="token punctuation">)</span><br><br><span class="token comment"># pad sequences to the maximum length of all sentences</span><br>max_sequence_len <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> input_sequences<span class="token punctuation">]</span><span class="token punctuation">)</span><br>input_sequences <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pad_sequences<span class="token punctuation">(</span>input_sequences<span class="token punctuation">,</span> maxlen<span class="token operator">=</span>max_sequence_len<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'pre'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><br><span class="token comment"># create predictors and label</span><br><span class="token comment"># [0,0,1,2] -&gt; 2 is label</span><br><span class="token comment"># [0,1,2,3] -&gt; 3 is label</span><br><span class="token comment"># [1,2,3,4] -&gt; 4 is label</span><br>xs<span class="token punctuation">,</span> labels <span class="token operator">=</span> input_sequences<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>input_sequences<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><br><br><span class="token comment"># one-hot encoding the labels (classification problem)</span><br>ys <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>to_categorical<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> num_classes<span class="token operator">=</span>total_words<span class="token punctuation">)</span></code></pre><pre class="language-python"><code class="language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Embedding<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span>max_sequence_len<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># take only 20 units (bi-direction) to train</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> ys<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><pre class="language-python"><code class="language-python">seed_text <span class="token operator">=</span> <span class="token string">"Laurence went to dublin"</span><br>next_words <span class="token operator">=</span> <span class="token number">100</span><br><br><span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>next_words<span class="token punctuation">)</span><span class="token punctuation">:</span><br>	token_list <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span>seed_text<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><br>	<span class="token comment"># "went to dublin" -&gt; [134, 13, 59]</span><br>	token_list <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span>token_list<span class="token punctuation">]</span><span class="token punctuation">,</span> maxlen<span class="token operator">=</span>max_sequence_len<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'pre'</span><span class="token punctuation">)</span><br>	<span class="token comment">#  [0, 0, 0, 0, 0, 0, 0, 134, 13, 59]</span><br>	predicted <span class="token operator">=</span> model<span class="token punctuation">.</span>predict_classes<span class="token punctuation">(</span>token_list<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><br>	output_word <span class="token operator">=</span> <span class="token string">""</span><br>	<span class="token comment"># revert an index back to the word</span><br>	<span class="token keyword">for</span> word<span class="token punctuation">,</span> index <span class="token keyword">in</span> tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><br>		<span class="token keyword">if</span> index <span class="token operator">==</span> predicted<span class="token punctuation">:</span><br>			output_word <span class="token operator">=</span> word<br>			<span class="token keyword">break</span><br>	<span class="token comment"># add predicted word to the seed text and make another prediction</span><br>	seed_text <span class="token operator">+=</span> <span class="token string">" "</span> <span class="token operator">+</span> output_word<br><span class="token keyword">print</span><span class="token punctuation">(</span>seed_text<span class="token punctuation">)</span><br><span class="token comment"># all the words are predicted based on the probability</span><br><span class="token comment"># next one will be less certain than the previous</span><br><span class="token comment"># -&gt; less meaningful</span></code></pre><ul><li>Using more words will help.</li></ul><p>👉 <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_3_more_data_on_train.html">Notebook 3 (more data)</a></p><pre class="language-python"><code class="language-python"><span class="token comment"># read from a file</span><br>tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span><span class="token punctuation">)</span><br>data <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'/tmp/irish-lyrics-eof.txt'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><br>corpus <span class="token operator">=</span> data<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span></code></pre><p>A little changes from the previous,</p><pre class="language-python"><code class="language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Embedding<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span>max_sequence_len<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span><span class="token number">150</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>adam <span class="token operator">=</span> Adam<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span> <span class="token comment"># customized optimizer</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>adam<span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br><span class="token comment">#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')</span><br>history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> ys<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><ul><li>Different convernges can create different poetry.</li><li>If we use one-hot for a very big corpus -&gt; take a lot of RAM -&gt; use <strong>character-based prediction</strong> -&gt; #unique characters is far less than #unique words. -&gt; <a href="https://www.tensorflow.org/tutorials/text/text_generation">notebook "Text generation with RNN"</a></li></ul><p>👉 Notebook <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_4_using_lstm_write_shakespeare.html">Using LSTMs, see if you can write Shakespeare!</a></p></div><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"TF 3 - NLP in TensorFlow","image":["https://dinhanhthi.com/img/post/mooc/tf/rnn-basic-idea.png","https://dinhanhthi.com/img/post/mooc/tf/lstm-basic-idea.png","https://dinhanhthi.com/img/post/mooc/tf/1layer-vs-2layer-lstm.png","https://dinhanhthi.com/img/post/mooc/tf/with-vs-without-lstm.png","https://dinhanhthi.com/img/post/mooc/tf/using-conv-net.png"],"author":"Anh-Thi DINH","genre":"Insert a schema.org genre","url":"https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/","mainEntityOfPage":"https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/","datePublished":"13-09-2020","dateModified":"03-12-2020","description":"This is my note for the 3rd course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..."}</script></article></main><footer><a href="/" target="_blank">Thi &nbsp;©&nbsp; 2020 </a>&nbsp;•&nbsp; <a href="/about-the-notes/">About the notes </a>&nbsp;•&nbsp; <a href="https://pobo.dinhanhthi.com" target="_blank">Po Bo </a>&nbsp;•&nbsp; <a href="/for-me-only/">For me only </a>&nbsp;•&nbsp; <a href="/donate/">Support Thi</a></footer></body></html>