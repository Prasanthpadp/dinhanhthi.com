<!DOCTYPE html><html domain=.com lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="default-src 'self';object-src 'none';script-src 'self' 'unsafe-inline' https://cdnjs.cloudflare.com/;style-src 'self' https://fonts.googleapis.com/ https://use.fontawesome.com/ 'unsafe-inline';img-src 'self' data:;font-src 'self' https://fonts.gstatic.com/ https://use.fontawesome.com/" http-equiv=Content-Security-Policy><link href=/favicon.svg rel=icon type=image/svg+xml><meta content=#f9c412 name=theme-color><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name=robots><title>TF 3 - NLP in TensorFlow</title><meta content="TF 3 - NLP in TensorFlow" property=og:title><meta content="This is my note for the 3rd course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..." name=description><meta content="This is my note for the 3rd course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..." property=og:description><meta content=summary_large_image name=twitter:card><meta content=@dinhanhthi name=twitter:site><meta content=@dinhanhthi name=twitter:creator><meta content=/img/header/tensorflow.svg property=og:image><link href=https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/ rel=canonical><meta content=no-referrer-when-downgrade name=referrer><link href=/feed/feed.xml rel=alternate type=application/atom+xml title="ðŸ”¥ Anh-Thi DINH"><link href=/ rel=preconnect crossorigin=""><script src="/js/min.js?hash=70bdd86fac" async defer></script><script csp-hash="">if (/Mac OS X/.test(navigator.userAgent))
      document
        .documentElement
        .classList
        .add('apple')</script><style>@charset "UTF-8";@font-face{font-display:swap;font-family:"Poppins";font-style:normal;font-weight:400;src:url(/fonts/poppins/poppins-v15-latin-regular.eot);src:local("Poppins Regular"),local("Poppins-Regular"),url(/fonts/poppins/poppins-v15-latin-regular.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-regular.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-regular.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-regular.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-regular.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:"Poppins";font-style:italic;font-weight:400;src:url(/fonts/poppins/poppins-v15-latin-italic.eot);src:local("Poppins Italic"),local("Poppins-Italic"),url(/fonts/poppins/poppins-v15-latin-italic.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-italic.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-italic.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-italic.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-italic.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:'Poppins';font-style:normal;font-weight:600;src:url(/fonts/poppins/poppins-v15-latin-600.eot);src:local("Poppins SemiBold"),local("Poppins-SemiBold"),url(/fonts/poppins/poppins-v15-latin-600.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-600.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-600.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-600.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-600.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:'Poppins';font-style:italic;font-weight:600;src:url(/fonts/poppins/poppins-v15-latin-600italic.eot);src:local("Poppins SemiBold Italic"),local("Poppins-SemiBoldItalic"),url(/fonts/poppins/poppins-v15-latin-600italic.eot?#iefix) format("embedded-opentype"),url(/fonts/poppins/poppins-v15-latin-600italic.woff2) format("woff2"),url(/fonts/poppins/poppins-v15-latin-600italic.woff) format("woff"),url(/fonts/poppins/poppins-v15-latin-600italic.ttf) format("truetype"),url(/fonts/poppins/poppins-v15-latin-600italic.svg#Poppins) format("svg")}@font-face{font-display:swap;font-family:'Recoleta';src:url(/fonts/recoleta/Recoleta-Bold.woff2) format("woff2"),url(/fonts/recoleta/Recoleta-Bold.woff) format("woff"),url(/fonts/poppins/Recoleta-Bold.ttf) format("truetype");font-weight:700;font-style:bold}*{border:0;box-sizing:border-box}:root{font-size:16.5px}main img{content-visibility:auto}::-webkit-scrollbar{height:10px;width:10px}::-webkit-scrollbar-thumb{background:#3e466b;border-radius:6px}::-webkit-scrollbar-thumb:hover{background:#aaa;cursor:pointer}::-webkit-scrollbar-track{background:#363948}::-webkit-scrollbar-corner{background:#282a36}body,html{font-family:"Poppins",Arial,Helvetica,sans-serif;font-size:16.5px}html{-webkit-text-size-adjust:100%}@supports (font-variation-settings:normal){html{font-family:"Poppins" var alt,Arial,Helvetica,sans-serif}}body{background:#282a36;color:#eee;margin:0;counter-reset:h2counter}strong{font-weight:600;color:#8cc8ff}small{font-size:80%}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}mark{color:inherit;padding:0;background-color:transparent;box-shadow:inset 0 -4px 0 0 #ffd479}a,a:hover{text-decoration:none}a:hover{border-bottom:2px solid #ffd479;color:#fff}a.no-effect:hover{border-bottom:none}img{max-width:100%}main{margin:0 auto}.page-note h3,.page-note>h2,h1,h2,h3{font-family:"Recoleta",Arial,Helvetica,sans-serif}h1{font-size:29.7px}h2{font-size:24.75px}h3{font-size:21.5325px}.page-note h3,.page-note>h2{color:#eee}.page-note h3:hover .direct-link,.page-note>h2:hover .direct-link{display:inline-block}.page-note h3:hover a:hover,.page-note>h2:hover a:hover,a{color:#ffd479}.page-note>h2{font-size:24.75px;margin:37.125px 0 19.8px}.page-note p+ol,.page-note p+ul,.page-note>h2+h3{margin-top:-.5rem}.page-note>h3{font-size:21.5325px;margin-bottom:19.8px}.page-note .direct-link{display:none;color:#777;border-bottom:none;margin-left:3px}.page-note pre+h2,.page-note pre+h3,.page-note>h3{margin-top:37.125px}.page-note h1{counter-reset:h2counter}.page-note h2{counter-reset:h3counter}.page-note h3{counter-reset:h4counter}.page-note h3:before{opacity:.5;content:counter(h2counter) "." counter(h3counter) ".Â Â ";counter-increment:h3counter}.page-note h2:before{content:counter(h2counter) ".Â Â ";counter-increment:h2counter;color:#8cc8ff}.container,header{width:100%;margin:0 auto}.normal{padding:0 16.5px;width:100%}@media (min-width:916.5px){.normal{width:900px}}.mt-2{margin-top:2rem}.page-index .container{padding:2rem 1rem 0}.page-index .main-cats{flex:0 1 calc(100% - 280px);padding-right:1rem}.page-index .main-cats>.category-wrapper{padding-bottom:1.5rem}.page-index .main-cats>.category-wrapper>.category{border:1px solid #3b3e54;border-radius:7px;height:fit-content;background:#2f3240;padding:2rem 1.5rem 1.5rem}.page-index .toc-index{border:1px solid #404040;border-radius:7px;height:fit-content;background:#35373c;flex:0 1 280px;position:sticky;top:60px;padding:1rem 1.1rem}.page-index .toc-index h3{padding-bottom:5px;border-bottom:1px solid #555;margin:0 0 10px;font-size:1.3rem}.page-index .toc-index ul{padding-left:20px;margin:0}.page-index .toc-index p{font-style:italic;color:#999;padding-top:0;margin-bottom:0;font-size:.95rem;margin-top:10px}.page-index .toc-index p a{color:#999;border-bottom:2px solid #999}.page-index .toc-index p a:hover{border-bottom:2px solid #ffd479}@media (max-width:991px){.page-index .main-cats{flex:1 1 100%;order:2;padding-right:0}.page-index .toc-index{flex:1 1 100%;order:1;position:inherit;margin-bottom:1.5rem}.page-index .toc-index ul{column-count:3;-webkit-column-count:3;-moz-column-count:3}}@media (max-width:767px){.page-index .toc-index ul{column-count:2;-webkit-column-count:2;-moz-column-count:2}}@media (max-width:575px){.page-index .toc-index ul{column-count:1;-webkit-column-count:1;-moz-column-count:1}}.category{width:100%}.category h2,header h1{font-size:1.55rem;margin-top:0}.category h2 img{float:left;margin-right:7px}.category .list-homepage{list-style:none;padding-left:10px;margin-bottom:0;column-count:1;-webkit-column-count:1;-moz-column-count:1}@media (min-width:768px){.category .list-homepage{column-count:2;-webkit-column-count:2;-moz-column-count:2}}@media (min-width:992px){.category .list-homepage{column-count:3;-webkit-column-count:3;-moz-column-count:3}}.category .list-homepage li{padding-left:15px;margin-bottom:10px;display:inline-block;width:100%}.category .list-homepage li a{color:#ddd;border-bottom:2px solid rgba(255,255,255,.14);border-style:dotted}.category .list-homepage li::before{content:"ðŸ“„";margin-right:5px;margin-left:-25px;opacity:.8}.category .list-homepage li:hover{cursor:pointer}.category .list-homepage li:hover a{border-color:#ffd479;color:#fff}.category .list-homepage li:hover::before{opacity:1}.page-index header{padding-top:5em;padding-bottom:0}.page-index header .header-logo{height:80px;width:auto}header nav{z-index:10}#nav{z-index:2;position:relative}header{padding:5.5rem 1.5rem 1rem;width:900px;text-align:center;max-width:100%;display:flex;align-items:center;flex-direction:column}header h1{font-size:2.2rem;margin-bottom:0}header p{margin-top:1rem}header .header-logo{width:55px;height:55px;margin-bottom:1rem}header .header-logo img{width:100%;height:100%}header .social{margin-top:.5rem}header #more-info #note-tag>a,header .social a{margin-right:10px}header .social a:last-child{margin-right:0}@media (min-width:992px){header .social a{margin-right:20px}}header .social a img{border-radius:50%}header #more-info{padding:1rem}header #more-info #note-tag{padding-bottom:10px;border-bottom:1px solid #444}header #more-info #note-tag>a::before{content:"#"}header #more-info #last-modified{padding-top:10px;font-style:italic}#reading-progress,nav{top:0;left:0;width:100vw}#reading-progress{z-index:3;border-bottom:1px solid #ffd479;position:absolute;bottom:0;transform:translate(-100vw,0);will-change:transform;pointer-events:none}.intro,.job span{font-size:1.02rem}.intro b,.intro strong{color:#ffd479;font-weight:400}.intro a,footer a:hover{color:#fff}.job span{background:#ffd479;color:#000;padding:3px 10px;border-radius:15px}nav{position:fixed;padding:0 1.5em;background:#35373c}@media (max-width:992px){nav{padding:0 1em}}@media (max-width:576px){nav{padding:0 .5em}}nav #nav{display:-ms-flexbox;-ms-flex-align:center;align-items:center}nav #nav a{color:#ccc;margin-right:15px;align-items:center;padding:0 .5rem;font-size:1.1rem;white-space:nowrap}nav #nav a img{margin-right:5px}nav #nav a:hover{color:#fff;cursor:pointer;text-decoration:none}nav #nav .nav-item{text-align:left;margin-right:5px}@media (min-width:421px){nav #nav .nav-item{padding-left:0!important}}@media (max-width:575px){nav #nav .nav-item{width:unset!important}nav #nav .nav-item span{display:none}}@media (min-width:576px){nav #nav .nav-item{margin-right:15px}}nav #nav .nav-github{text-align:right;padding-right:0!important;margin-right:0!important}nav #nav .nav-search{display:block;background-image:linear-gradient(to right,#3d4251,#3b3f4c,#393d46,#373a41,#35373c);width:100%;position:relative}nav #nav,nav #nav .nav-search form,nav #nav a{display:flex}nav #nav .nav-search .nav-search-input{border:0;background:0 0;color:#ddd;font-size:1.05rem;padding:.65rem .5rem;width:100%}nav #nav .nav-search .nav-search-input:focus{outline:0;border:0}nav #nav .nav-search #search-result{position:absolute;max-height:80vh;overflow:auto;width:100%;background:#35373c;border-bottom-right-radius:5px;border-bottom-left-radius:5px;padding:0 1rem}@media (max-width:767px){nav #nav .nav-search #search-result{position:fixed;left:0;right:0;border-radius:0}}nav #nav .nav-search #search-result ul#searchResults{padding:0 0 0 20px}.page-note ol li,.page-note ul li,div.toc ol li,nav #nav .nav-search #search-result ul#searchResults li h3{margin-bottom:5px}nav #nav .nav-search #search-result ul#searchResults li h3 a{color:#efb232;white-space:inherit;padding:0;text-align:left;font-weight:400;font-size:16.5px;font-family:"Poppins",Arial,Helvetica,sans-serif;line-height:1.4}nav #nav .nav-search #search-result ul#searchResults li h3 a:hover{border-bottom:none;color:#8cc8ff}nav #nav .nav-search #search-result ul#searchResults li p{text-align:left;margin-top:0;line-height:1.4}nav #nav .nav-search #search-result #noResultsFound p{text-align:left}footer{font-size:1.1rem;background:#35373c;padding:.75rem 1rem;text-align:center;margin-top:3rem}footer a{color:#ccc}.info{position:relative;padding:16.5px;margin-bottom:24.75px;border:1px solid transparent;border-radius:.25rem;color:#eee;border-left-width:15px;border-color:#8cc8ff}.info :last-child{margin-bottom:0}.hsbox{margin-bottom:24.75px;border:1px solid #969696;padding:1rem;border-radius:3px}.hsbox .hs__title{cursor:pointer}.hsbox .hs__title::before{content:" ";display:inline-block;border-top:7px solid transparent;border-bottom:7px solid transparent;border-left:7px solid currentColor;vertical-align:middle;margin-right:.7rem;transform:translateY(-2px);transition:transform .2s ease-out}.hsbox .hs__title.show{padding-bottom:15px;border-bottom:.5px solid #666;margin-bottom:1rem}.hsbox .hs__title.show+.hs__content{display:block;opacity:1;padding:5px 0;transition:all .25s 0s cubic-bezier(.4,0,.2,1)}.hsbox .hs__title.show::before{transform:rotate(90deg) translateX(-3px)}.hsbox .hs__content{display:none;transition:all .2s 0s ease}.hsbox .hs__content>:last-child{margin-bottom:0}.page-note .text-center{text-align:center}.page-note ol,.page-note p,.page-note ul{margin-top:0;margin-bottom:24.75px}.page-note ol li ol,.page-note ol li ul,.page-note ul li ol,.page-note ul li ul{margin-bottom:5px;padding-left:20px}.page-note ol li>*,.page-note ul li>*{margin-bottom:10px}.page-note p.noindent{display:none;padding-left:20px}.page-note p.noindent+ol,.page-note p.noindent+ul{padding-left:20px}.page-note p.indent{display:none;padding-left:40px}.page-note ol.indent,.page-note p.indent+ol,.page-note p.indent+ul,.page-note ul.indent{padding-left:40px}.page-note ol.noindent,.page-note ul.noindent{padding-left:20px}.page-note hr{border-bottom:1px solid #676767;margin-bottom:24.75px}#reading-list .item .author{font-style:italic}#reading-list .item .intro{color:#999}code[class*=language-],pre[class*=language-]{color:#eee;font-size:16px;text-shadow:none;font-family:Menlo,Monaco,Consolas,"Andale Mono","Ubuntu Mono","Courier New",monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{text-shadow:none;background:#75a7ca}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}.token.comment{color:#6a9955}.token.punctuation{color:#eee}.token.inserted,.token.number{color:#b5cea8}.token.builtin,.token.string{color:#ce9178}.token.operator{color:#eee}.token.keyword{color:#90cdff}.token.function{color:#efefac}.token.boolean,.token.important{color:#90cdff}.token.property{color:#9cdcfe}pre[class*=language-]>code[class*=language-]{position:relative;z-index:1}code,pre{font-family:Consolas,Menlo,Monaco,"Andale Mono WT","Andale Mono","Lucida Console","Lucida Sans Typewriter","DejaVu Sans Mono","Bitstream Vera Sans Mono","Liberation Mono","Nimbus Mono L","Courier New",Courier,monospace;line-height:1.5}h2>code{font-size:21.78px!important}h3>code{font-size:18.9486px!important}:not(pre)>code{border:1px solid #444;background:#6b444245;padding:2px 4px;margin:0 1px;border-radius:3px;font-size:.9rem;color:#fff;word-break:break-word}h2>code,h3>code{color:#ddd;padding-right:6px}a>code{color:#ffd479}a:hover>code{color:#ccc}pre,pre[class*=language-]{margin:0 0 27.225px;overflow:auto}pre>code,pre[class*=language-]>code{display:block;padding:14.5px 16.5px 16.5px;background:#2f3240;border:.5px solid #3b3e54;overflow:auto;border-radius:3px;max-height:450px}div.toc{margin-bottom:24.75px;border:1px solid #3b3e54;border-radius:7px;height:fit-content;background:#2f3240;padding:15px 15px 10px 0}div.toc>ol::before{content:"In this note";display:block;padding-bottom:5px;border-bottom:1px solid #555;margin:0 0 15px;font-size:1.18rem;font-family:"Recoleta",Arial,Helvetica,sans-serif}div.toc ol{padding-left:20px;font-size:14.85px;margin-bottom:0}div.toc ol li code{font-size:.85rem;background:#ececec;padding:0 4px 2px}div.toc ol li ol{padding-left:10px;margin-top:7px}div.toc ol,div.toc ol ol{counter-reset:item;list-style-type:none}div.toc ol li::before{content:counters(item,".") ". ";counter-increment:item}div.toc>ol>li ol>li::before{opacity:.7}div.toc>ol>li::before{color:#8cc8ff}@media (min-width:1300px){div.toc{float:right;margin-right:-280px;border-left:none;width:250px;position:-webkit-sticky;position:sticky;top:60px;max-height:70vh;overflow:auto}div.toc ol{margin-top:0;margin-bottom:0}}@media (min-width:1500px){div.toc{margin-right:-310px;width:280px}}.toc-active>a{color:#fff!important}.toc-active::before{opacity:1!important}.page-note img{height:auto;width:100%}@media (min-width:768px){.page-note .img-70,.page-note .img-75,.page-note .img-90{width:70%;margin-left:auto;margin-right:auto;display:block}.page-note .img-75,.page-note .img-90{width:75%}.page-note .img-90{width:90%}}.page-note p>img+br{display:none}.page-note p>img+br+em{display:block;text-align:center;margin-top:10px}</style><header><nav><div id=nav><a href=/ class="nav-item no-effect"><img alt=home src=/img/nav/home.svg height=18 width=18> <span>Thi</span> </a><a href=/about/ class="nav-item no-effect"><img alt=about src=/img/nav/about.svg height=15 width=15> <span>About</span></a><div class=nav-search><form><input aria-label="search notes..." class=nav-search-input id=searchField placeholder="search notes..." type=search></form><div id=search-result style="display: none;"><ul id=searchResults></ul><div id=noResultsFound style="display: none;"><p>No results found.</div></div></div><a href=https://github.com/dinhanhthi class="nav-item no-effect nav-github" target=_blank><img alt=github src=/img/nav/github.svg height=20 width=20></a></div><div id=reading-progress aria-hidden=true></div></nav><div class=header-logo><img alt="TF 3 - NLP in TensorFlow" src=/img/header/tensorflow.svg></div><h1>TF 3 - NLP in TensorFlow</h1><div id=more-info><div id=note-tag><a href=/tags/mooc>MOOC</a> <a href=/tags/deeplearning.ai>deeplearning.ai</a> <a href=/tags/deep-learning>Deep Learning</a> <a href=/tags/tensorflow>TensorFlow</a></div><div id=last-modified>03-12-2020 / <a href=https://github.com/dinhanhthi/dinhanhthi.com/edit/dev/./posts/mooc/2020-09-14-deeplearning-ai-tensorflow-course-3.md>Edit on Github</a></div></div></header><main><article><div class="container mt-2 normal page-note"><div class=toc><ol><li><a href=#tokernizing-%2B-padding>Tokernizing + padding</a><li><a href=#word-embeddings>Word embeddings</a><ol><li><a href=#imdb-review-dataset>IMDB review dataset</a><li><a href=#sarcasm-dataset>Sarcasm dataset</a></ol><li><a href=#pre-tokenized-datasets>Pre-tokenized datasets</a><li><a href=#sequence-models>Sequence models</a><ol><li><a href=#rnn-idea>RNN idea</a><li><a href=#lstm-idea>LSTM idea</a><li><a href=#with-vs-without-lstm>With vs without LSTM</a><li><a href=#using-a-convnet>Using a ConvNet</a><li><a href=#imdb-dataset>IMDB dataset</a></ol><li><a href=#sequence-models-and-literature>Sequence models and literature</a></ol></div><p>This is my note for the <a href=https://www.coursera.org/learn/natural-language-processing-tensorflow>3rd course</a> of <a href=https://www.coursera.org/specializations/tensorflow-in-practice>TensorFlow in Practice Specialization</a> given by <a href=http://deeplearning.ai/ >deeplearning.ai</a> and taught by Laurence Moroney on Coursera.<p>ðŸ‘‰ Check the codes <a href=https://github.com/dinhanhthi/deeplearning.ai-courses/tree/master/TensorFlow%20in%20Practice>on my Github</a>.<br>ðŸ‘‰ Official <a href=https://github.com/lmoroney/dlaicourse>notebooks</a> on Github.<p>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-1>course 1 - Intro to TensorFlow for AI, ML, DL</a>.<br>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-2>course 2 - CNN in TensorFlow</a>.<br>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-4>course 4 - Sequences, Time Series and Prediction</a>.<h2 id=tokernizing-%2B-padding>Tokernizing + padding <a href=#tokernizing-%2B-padding class=direct-link>#</a></h2><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-1/notebook_1_tokenizer_basic_examples.html>Tokenizer basic examples.</a><br>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-1/notebook_2_sarcasm_detection.html>Sarcasm detection</a>.<p class=noindent><ul><li>A common simple character encoding is ASCII,<li>We can encode each word as a number (token) -- <a href=https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer><code>Tokenizer</code></a>.<li>Tokenize words > build all the words to make a corpus > turn your sentences into lists of values based on these tokens. > manipulate these lists (make the same length, for example)</ul><pre class=language-python><code class=language-python><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer<br><br>sentences <span class="token operator">=</span> <span class="token punctuation">[</span><br>    <span class="token string">'i love my dog'</span><span class="token punctuation">,</span><br>    <span class="token string">'I, love my cat'</span><span class="token punctuation">,</span><br>    <span class="token string">'You love my dog so much!'</span><br><span class="token punctuation">]</span><br><br>tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>num_words <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> oov_token<span class="token operator">=</span><span class="token string">"&lt;OOV>"</span><span class="token punctuation">)</span><br>            <span class="token comment"># num_words: max of words to be tokenized & pick</span><br>            <span class="token comment">#   the most common 100 words.</span><br>            <span class="token comment"># More words, more accuracy, more time to train</span><br>            <span class="token comment"># oov_token: replace unseen words by "&lt;OOV>"</span><br>tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span> <span class="token comment"># fix texts based on tokens</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># indexing words</span><br>word_index <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>word_index<br><span class="token keyword">print</span><span class="token punctuation">(</span>word_index<span class="token punctuation">)</span><br><span class="token comment"># {'&lt;OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7, 'so': 8, 'much': 9}</span><br><span class="token comment"># "!", ",", capital, ... are removed</span></code></pre><p>ðŸ‘‰ <a href=https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer>tf.keras.preprocessing.text.Tokenizer</a><pre class=language-python><code class=language-python><span class="token comment"># encode sentences</span><br>sequences <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>sequences<span class="token punctuation">)</span><br><span class="token comment"># [[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5, 8, 9]]</span><br><span class="token comment"># if a word is not in the word index, it will be lost in the text_to_sequences()</span></code></pre><p>ðŸ‘‰ <a href=https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences>tf.keras.preprocessing.sequence.pad_sequences</a><pre class=language-python><code class=language-python><span class="token comment"># make encoded sentences equal</span><br><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>sequence <span class="token keyword">import</span> pad_sequences<br><br>padded <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><br>                       maxlen<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"post"</span><span class="token punctuation">,</span> truncating<span class="token operator">=</span><span class="token string">"post"</span><span class="token punctuation">)</span><br>         <span class="token comment"># maxlen: max len of encoded sentence</span><br>         <span class="token comment"># value: value to be filld (default 0)</span><br>         <span class="token comment"># padding: add missing values at beginning or ending of sentence?</span><br>         <span class="token comment"># truncating: longer than maxlen? cut at beginning or ending?</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>padded<span class="token punctuation">)</span><br><span class="token comment"># [[ 4  2  3  5 -1]</span><br><span class="token comment">#  [ 4  2  3  6 -1]</span><br><span class="token comment">#  [ 7  2  3  5  8]]</span></code></pre><p>ðŸ‘‰ <a href=https://rishabhmisra.github.io/publications/ >Sarcasm detection dataset.</a><pre class=language-python><code class=language-python><span class="token comment"># read json text</span><br><span class="token keyword">import</span> json<br><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"/tmp/sarcasm.json"</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span><br>    datastore <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span><br><br>sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br>labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br>urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br><span class="token keyword">for</span> item <span class="token keyword">in</span> datastore<span class="token punctuation">:</span><br>    sentences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'headline'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>    labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'is_sarcastic'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>    urls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'article_link'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h2 id=word-embeddings>Word embeddings <a href=#word-embeddings class=direct-link>#</a></h2><p>ðŸ‘‰ <a href=https://projector.tensorflow.org/ >Embedding projector - visualization of high-dimensional data</a><br>ðŸ‘‰ <a href=http://ai.stanford.edu/~amaas/data/sentiment/ >Large Movie Review Dataset</a><h3 id=imdb-review-dataset>IMDB review dataset <a href=#imdb-review-dataset class=direct-link>#</a></h3><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_1_IMDB_reviews.html>Train IMDB review dataset</a>.<br>ðŸ‘‰ <a href=https://www.coursera.org/lecture/natural-language-processing-tensorflow/notebook-for-lesson-1-Q1Ln5>Video explain the code</a>.<p class=noindent><ul><li><strong>Word embeddings</strong> = the idea in which words and associated words are <em>clustered as vectors</em> in a multi-dimensional space. That allows words with similar meaning to have a similar representation.<li>The meaning of the words can come from labeling of the dataset.<ul><li><em>Example</em>: "dull" and "boring" show up a lot in negative reviews => they have similar sentiments => they are close to each other in the sentence => thus their vector will be similar => NN train + learn these vectors + associating them with the labels to come up with what's called in embedding.</ul><li>The purpose of <em>embedding dimension</em> is the number of dimensions for the vector representing the word encoding.</ul><pre class=language-python><code class=language-python><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<br><span class="token keyword">print</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span> <span class="token comment"># check version of tensorflow</span><br><br><span class="token comment"># If you are using tf1, you need below code</span><br>tf<span class="token punctuation">.</span>enable_eager_execution<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># IMDB reviews dataset</span><br><span class="token keyword">import</span> tensorflow_datasets <span class="token keyword">as</span> tfds<br>imdb<span class="token punctuation">,</span> info <span class="token operator">=</span> tfds<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"imdb_reviews"</span><span class="token punctuation">,</span> with_info<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> as_supervised<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br><br>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> imdb<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> imdb<span class="token punctuation">[</span><span class="token string">'test'</span><span class="token punctuation">]</span><br><br><span class="token keyword">for</span> s<span class="token punctuation">,</span>l <span class="token keyword">in</span> train_data<span class="token punctuation">:</span> <span class="token comment"># "s" for sentences "l" for labels</span><br>    <span class="token comment"># The values for "s" and "l" are tensors</span><br>    <span class="token comment"># so we need to extracr their values</span><br>    training_sentences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf8'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    training_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>l<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># Prepare for the NN</span><br>vocab_size <span class="token operator">=</span> <span class="token number">10000</span><br>embedding_dim <span class="token operator">=</span> <span class="token number">16</span> <span class="token comment"># embedding to dim 16</span><br>max_length <span class="token operator">=</span> <span class="token number">120</span> <span class="token comment"># of each sentence</span><br>trunc_type<span class="token operator">=</span><span class="token string">'post'</span> <span class="token comment"># cut the last words</span><br>oov_tok <span class="token operator">=</span> <span class="token string">"&lt;OOV>"</span> <span class="token comment"># replace not-encoded words by this</span><br><br><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer<br><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>sequence <span class="token keyword">import</span> pad_sequences<br><br>tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>num_words <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> oov_token<span class="token operator">=</span>oov_tok<span class="token punctuation">)</span><br>tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>training_sentences<span class="token punctuation">)</span><br>    <span class="token comment"># encoding the words</span><br>word_index <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>word_index<br>    <span class="token comment"># list of word index (built based on training set)</span><br>    <span class="token comment"># there may be many oov_tok in test set</span><br>sequences <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>training_sentences<span class="token punctuation">)</span><br>    <span class="token comment"># apply on sentences</span><br>padded <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span>maxlen<span class="token operator">=</span>max_length<span class="token punctuation">,</span> truncating<span class="token operator">=</span>trunc_type<span class="token punctuation">)</span><br>    <span class="token comment"># padding the sentences</span><br><br><span class="token comment"># apply to the test set</span><br>testing_sequences <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>testing_sentences<span class="token punctuation">)</span><br>testing_padded <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>testing_sequences<span class="token punctuation">,</span>maxlen<span class="token operator">=</span>max_length<span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># Simple NN</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> input_length<span class="token operator">=</span>max_length<span class="token punctuation">)</span><span class="token punctuation">,</span><br>                              <span class="token comment"># The result of embedding will be a 2D array:</span><br>                              <span class="token comment"># length of sentence x embedding_dim</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment"># Alternatively (a little diff on speed and accuracy):</span><br>    <span class="token comment"># tf.keras.layers.GlobalAveragePooling1D()</span><br>    <span class="token comment">#   average across the vectors to flatten it out</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'binary_crossentropy'</span><span class="token punctuation">,</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># Training</span><br>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>padded<span class="token punctuation">,</span> training_labels_final<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>testing_padded<span class="token punctuation">,</span> testing_labels_final<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># the result</span><br>e <span class="token operator">=</span> model<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token comment"># get the result of the embedding layers</span><br>weights <span class="token operator">=</span> e<span class="token punctuation">.</span>get_weights<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>weights<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># shape: (vocab_size, embedding_dim)</span></code></pre><p>If you wanna visualize the result (in 3D) with <a href=https://projector.tensorflow.org/ >Embedding projector</a>,<pre class=language-python><code class=language-python><span class="token keyword">import</span> io<br><br>out_v <span class="token operator">=</span> io<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'vecs.tsv'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><br>out_m <span class="token operator">=</span> io<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'meta.tsv'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><br><span class="token keyword">for</span> word_num <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">:</span><br>  word <span class="token operator">=</span> reverse_word_index<span class="token punctuation">[</span>word_num<span class="token punctuation">]</span><br>  embeddings <span class="token operator">=</span> weights<span class="token punctuation">[</span>word_num<span class="token punctuation">]</span><br>  out_m<span class="token punctuation">.</span>write<span class="token punctuation">(</span>word <span class="token operator">+</span> <span class="token string">"\n"</span><span class="token punctuation">)</span><br>  out_v<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> embeddings<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n"</span><span class="token punctuation">)</span><br>out_v<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><br>out_m<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br><span class="token keyword">try</span><span class="token punctuation">:</span><br>  <span class="token keyword">from</span> google<span class="token punctuation">.</span>colab <span class="token keyword">import</span> files<br><span class="token keyword">except</span> ImportError<span class="token punctuation">:</span><br>  <span class="token keyword">pass</span><br><span class="token keyword">else</span><span class="token punctuation">:</span><br>  files<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string">'vecs.tsv'</span><span class="token punctuation">)</span><br>  files<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string">'meta.tsv'</span><span class="token punctuation">)</span></code></pre><h3 id=sarcasm-dataset>Sarcasm dataset <a href=#sarcasm-dataset class=direct-link>#</a></h3><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_2_sacarsm.html>Train Sacarsm dataset</a>.<ul><li>In text data, it usually happens that the accuracy increase over the number of training but the loss increase sharply also. We can "play" with hyperparameter to see the effect.</ul><pre class=language-python><code class=language-python><span class="token comment"># Run this to ensure TensorFlow 2.x is used</span><br><span class="token keyword">try</span><span class="token punctuation">:</span><br>  <span class="token comment"># %tensorflow_version only exists in Colab.</span><br>  <span class="token operator">%</span>tensorflow_version <span class="token number">2.</span>x<br><span class="token keyword">except</span> Exception<span class="token punctuation">:</span><br>  <span class="token keyword">pass</span></code></pre><h2 id=pre-tokenized-datasets>Pre-tokenized datasets <a href=#pre-tokenized-datasets class=direct-link>#</a></h2><p>ðŸ‘‰ <a href=https://github.com/tensorflow/datasets/blob/master/docs/catalog/imdb_reviews.md>datasets/imdb_reviews.md at master Â· tensorflow/datasets</a><br>ðŸ‘‰ <a href=https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder>tfds.features.text.SubwordTextEncoder Â |Â  TensorFlow Datasets</a><br>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_3_pre-tokenizer.html>Pre-tokenizer example</a>.<br>ðŸ‘‰ <a href=https://www.coursera.org/lecture/natural-language-processing-tensorflow/notebook-for-lesson-3-piQXt>Video exaplain the codes</a>.<ul><li>There are someones who did the work (tokenization) for you.<li>Try on IMDB dataset that has been pre-tokenized.<li>The tokenization is done on <strong>subwords</strong>!<li>The sequence of words can be just important as their existence.</ul><pre class=language-python><code class=language-python><span class="token comment"># load imdb dataset from tensorflow</span><br><span class="token keyword">import</span> tensorflow_datasets <span class="token keyword">as</span> tfds<br>imdb<span class="token punctuation">,</span> info <span class="token operator">=</span> tfds<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"imdb_reviews/subwords8k"</span><span class="token punctuation">,</span> with_info<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> as_supervised<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br><br><span class="token comment"># extract train/test sets</span><br>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> imdb<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> imdb<span class="token punctuation">[</span><span class="token string">'test'</span><span class="token punctuation">]</span><br><br><span class="token comment"># take the tokernizer</span><br>tokenizer <span class="token operator">=</span> info<span class="token punctuation">.</span>features<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>encoder<br><br><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>subwords<span class="token punctuation">)</span><br><span class="token comment"># ['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_',...</span></code></pre><pre class=language-python><code class=language-python>sample_string <span class="token operator">=</span> <span class="token string">'TensorFlow, from basics to mastery'</span><br><br>tokenized_string <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>sample_string<span class="token punctuation">)</span><br><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'Tokenized string is {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>tokenized_string<span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token comment"># Tokenized string is [6307, 2327, 4043, 2120, 2, 48, 4249, 4429, 7, 2652, 8050]</span><br><br>original_string <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>tokenized_string<span class="token punctuation">)</span><br><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'The original string: {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>original_string<span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token comment"># The original string: TensorFlow, from basics to mastery</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># take a look on tokenized string</span><br><span class="token comment"># case sensitive + punctuation maintained</span><br><span class="token keyword">for</span> ts <span class="token keyword">in</span> tokenized_string<span class="token punctuation">:</span><br>  <span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'{} ----> {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>ts<span class="token punctuation">,</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span>ts<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><br><span class="token comment"># 6307 ----> Ten</span><br><span class="token comment"># 2327 ----> sor</span><br><span class="token comment"># 4043 ----> Fl</span><br><span class="token comment"># ...</span></code></pre><ul><li>The code run quite long (4 minutes each epoch if using GPU on colab) because there are a lot of hyperparameters and sub-words.<li>Result: 50% acc & loss is decreasing but very small.<ul><li>Because we are using sub-words, not for-words -> they (sub-words) are nonsensical. -> they are only when we put them together in sequences -> <strong>learning from sequences would be a great way forward</strong> -> <strong>RNN</strong> (Recurrent Neural Networks)</ul></ul><h2 id=sequence-models>Sequence models <a href=#sequence-models class=direct-link>#</a></h2><ul><li>The relative ordering, the sequence of words, matters for the meaning of the sentence .<li>For NN to take into account for the <strong>ordering of the words</strong>: <strong>RNN</strong> (Recurrent Neural Networks), <strong>LSTM</strong> (Long short-term memory).<li><strong>Why not RNN but LSTM ?</strong> With RNN, the context is preserved from timstamp to timestamp BUT that may get lost in longer sentences => LSTM gets better because it has cell state.<li><strong>Example of using LSTM</strong>: "<em>I grew up in Ireland, I went to school and at school, they made me learn how to speak...</em>" => "speak" is the context and we go back to the beginning to catch "Ireland", then the next word could be "leanr how to speak <strong>Gaelic</strong>"!</ul><h3 id=rnn-idea>RNN idea <a href=#rnn-idea class=direct-link>#</a></h3><p>ðŸ‘‰ <a href=/deeplearning-ai-course-5>Note of the course of sequence model</a>.<p class=noindent><ul><li>The usual NN, something like "f(data, labels)=rules" cannot take into account of sequences.<li><strong>An example of using sequences</strong>: Fibonacci sequence => the result of current function is the input of next function itself,...</ul><p><img alt="RNN basic idea" src=/img/post/mooc/tf/rnn-basic-idea.png class="pop img-70"><br><em>RNN basic idea (<a href=https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359>source</a>).</em><h3 id=lstm-idea>LSTM idea <a href=#lstm-idea class=direct-link>#</a></h3><p>ðŸ‘‰ (Video) <a href="https://www.youtube.com/watch?v=8HyCNIVRbSU&feature=emb_title">Illustrated Guide to LSTM's and GRU's: A step by step explanation</a> & <a href=https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21>its article</a>.<p class=noindent><ul><li>Sometimes, the sequence context leads to lose information like the example of "Ireland" and "Gaelic" before.<li>LSTM has an additional pipeline called <strong>Cell State</strong>. It can pass through the network to impact it + help to keep context from earlier tokens relevance.</ul><p><img alt="LSTM basic idea" src=/img/post/mooc/tf/lstm-basic-idea.png class="pop img-75"><br><em>LSTM basic idea (image from the course).</em><pre class=language-python><code class=language-python><span class="token comment"># SINGLE LAYER LSTM</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>      <span class="token comment"># 64: #oututs desired (but the result may be different)</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_1_IMDB_subwords_8K_with_single_layer_LSTM.html>IMDB Subwords 8K with Single Layer LSTM</a><pre class=language-python><code class=language-python><span class="token comment"># MULTI PLAYER LSTM</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>      <span class="token comment"># return_sequences=True: required if we wanna feed LSTM into another one</span><br>      <span class="token comment"># It ensures that the output of LSTM match the desired inputs of the next one</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_2_IMDB_subwords_8K_with_multi_layer_LSTM.html>IMDB Subwords 8K with Multi Layer LSTM</a><p><img alt="1layer vs 2 later LSTM acc" src=/img/post/mooc/tf/1layer-vs-2layer-lstm.png class="pop img-90"><br><em>1 layer vs 2 layer LSTM accuracy after 50 epochs (image from the course). 2 layer is better (smoother) which makes us more confident about the model. The validation acc is sticked to 80% because we used 8000 sub-words taken from training set, so there may be many tokens from the test set that would be out of vocabulary.</em><h3 id=with-vs-without-lstm>With vs without LSTM <a href=#with-vs-without-lstm class=direct-link>#</a></h3><pre class=language-python><code class=language-python><span class="token comment"># WITHOUT LSTM (like previous section)</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span><br>                              input_length<span class="token operator">=</span>max_length<span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GlobalmaxPooling1D<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># WITH LSTM</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span><br>                              input_length<span class="token operator">=</span>max_length<span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><img alt="With vs without LSTM" src=/img/post/mooc/tf/with-vs-without-lstm.png class="pop img-90"><br><em>With vs without LSTM (image from the course). With LSTM is really better but there is still overfitting here.</em><h3 id=using-a-convnet>Using a ConvNet <a href=#using-a-convnet class=direct-link>#</a></h3><p>ðŸ‘‰ <a href=https://www.coursera.org/lecture/natural-language-processing-tensorflow/using-a-convolutional-network-fSE8o>Video explains the dimension</a>.<br>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_3_IMDB_subwords_8K_with_Conv.html>IMDB Subwords 8K with 1D Convolutional Layer</a>.<pre class=language-python><code class=language-python>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv1D<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GlobalAveragePooling1D<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><img alt="Using Convolution network." src=/img/post/mooc/tf/using-conv-net.png class="pop img-90"><br><em>Using Convolution network. (image from the course). It's really better but there is overfitting there.</em><h3 id=imdb-dataset>IMDB dataset <a href=#imdb-dataset class=direct-link>#</a></h3><p>ðŸ‘‰ Notebook: <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_4_IMDB_review_with_GRU.html>IMDB Reviews with GRU (and optional LSTM and Conv1D)</a>.<br>ðŸ‘‰ <a href=https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/NFvFd/going-back-to-the-imdb-dataset>Video compares the results</a>.<p>Try with 3 different choices:<p class=indent><ul><li><strong>Simple NN</strong>: 5s/epoch, 170K params, nice acc, overfitting.<li><strong>LSTM</strong>: 43s/epoch, 30K params, acc better, overfitting.<li><strong>GRU</strong> (Gated Recurrent Unit layer, a different type of RNN): 20s/epoch, 169K params, very good acc, overfitting.<li><strong>Conv1D</strong>: 6s/epoch, 171K params, good acc, overfitting.</ul><p><strong>Remark</strong>: <mark>With the texts, you'll probably get a bit more overfitting than you would have done with images.</mark> Because we have out of voca words in validation data.<h2 id=sequence-models-and-literature>Sequence models and literature <a href=#sequence-models-and-literature class=direct-link>#</a></h2><p>One application of sequence models: read text then <strong>generate another look-alike text</strong>.<p>ðŸ‘‰ <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_1_find_the_next_word_trained_from_a_song.html>Notebook 1</a> & <a href=https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/B80b0/notebook-for-lesson-1>explaining video</a>.<ul><li>How they predict a new word in the notebook? -> Check <a href=https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/LGBS2/predicting-a-word>this video</a>.</ul><pre class=language-python><code class=language-python>input_sequences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br><span class="token keyword">for</span> line <span class="token keyword">in</span> corpus<span class="token punctuation">:</span><br>	<span class="token comment"># convert each sentence to list of numbers</span><br>	token_list <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span>line<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><br>	<span class="token comment"># convert each list to n-gram sequence</span><br>	<span class="token comment"># eg. from [1,2,3,4,5]</span><br>	<span class="token comment"># 		to [1,2], [1,2,3], [1,2,3,4], [1,2,3,4,5]</span><br>	<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>token_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><br>		n_gram_sequence <span class="token operator">=</span> token_list<span class="token punctuation">[</span><span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><br>		input_sequences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>n_gram_sequence<span class="token punctuation">)</span><br><br><span class="token comment"># pad sequences to the maximum length of all sentences</span><br>max_sequence_len <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> input_sequences<span class="token punctuation">]</span><span class="token punctuation">)</span><br>input_sequences <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pad_sequences<span class="token punctuation">(</span>input_sequences<span class="token punctuation">,</span> maxlen<span class="token operator">=</span>max_sequence_len<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'pre'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><br><span class="token comment"># create predictors and label</span><br><span class="token comment"># [0,0,1,2] -> 2 is label</span><br><span class="token comment"># [0,1,2,3] -> 3 is label</span><br><span class="token comment"># [1,2,3,4] -> 4 is label</span><br>xs<span class="token punctuation">,</span> labels <span class="token operator">=</span> input_sequences<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>input_sequences<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><br><br><span class="token comment"># one-hot encoding the labels (classification problem)</span><br>ys <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>to_categorical<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> num_classes<span class="token operator">=</span>total_words<span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python>model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Embedding<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span>max_sequence_len<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># take only 20 units (bi-direction) to train</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> ys<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python>seed_text <span class="token operator">=</span> <span class="token string">"Laurence went to dublin"</span><br>next_words <span class="token operator">=</span> <span class="token number">100</span><br><br><span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>next_words<span class="token punctuation">)</span><span class="token punctuation">:</span><br>	token_list <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span>seed_text<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><br>	<span class="token comment"># "went to dublin" -> [134, 13, 59]</span><br>	token_list <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span>token_list<span class="token punctuation">]</span><span class="token punctuation">,</span> maxlen<span class="token operator">=</span>max_sequence_len<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'pre'</span><span class="token punctuation">)</span><br>	<span class="token comment">#  [0, 0, 0, 0, 0, 0, 0, 134, 13, 59]</span><br>	predicted <span class="token operator">=</span> model<span class="token punctuation">.</span>predict_classes<span class="token punctuation">(</span>token_list<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><br>	output_word <span class="token operator">=</span> <span class="token string">""</span><br>	<span class="token comment"># revert an index back to the word</span><br>	<span class="token keyword">for</span> word<span class="token punctuation">,</span> index <span class="token keyword">in</span> tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><br>		<span class="token keyword">if</span> index <span class="token operator">==</span> predicted<span class="token punctuation">:</span><br>			output_word <span class="token operator">=</span> word<br>			<span class="token keyword">break</span><br>	<span class="token comment"># add predicted word to the seed text and make another prediction</span><br>	seed_text <span class="token operator">+=</span> <span class="token string">" "</span> <span class="token operator">+</span> output_word<br><span class="token keyword">print</span><span class="token punctuation">(</span>seed_text<span class="token punctuation">)</span><br><span class="token comment"># all the words are predicted based on the probability</span><br><span class="token comment"># next one will be less certain than the previous</span><br><span class="token comment"># -> less meaningful</span></code></pre><ul><li>Using more words will help.</ul><p>ðŸ‘‰ <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_3_more_data_on_train.html>Notebook 3 (more data)</a><pre class=language-python><code class=language-python><span class="token comment"># read from a file</span><br>tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span><span class="token punctuation">)</span><br>data <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'/tmp/irish-lyrics-eof.txt'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><br>corpus <span class="token operator">=</span> data<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span></code></pre><p>A little changes from the previous,<pre class=language-python><code class=language-python>model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Embedding<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span>max_sequence_len<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span><span class="token number">150</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>adam <span class="token operator">=</span> Adam<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span> <span class="token comment"># customized optimizer</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>adam<span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br><span class="token comment">#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')</span><br>history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> ys<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><ul><li>Different convernges can create different poetry.<li>If we use one-hot for a very big corpus -> take a lot of RAM -> use <strong>character-based prediction</strong> -> #unique characters is far less than #unique words. -> <a href=https://www.tensorflow.org/tutorials/text/text_generation>notebook "Text generation with RNN"</a></ul><p>ðŸ‘‰ Notebook <a href=https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_4_using_lstm_write_shakespeare.html>Using LSTMs, see if you can write Shakespeare!</a></div><script type=application/ld+json>{
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": "TF 3 - NLP in TensorFlow",
        "image": [],
        "author": "Anh-Thi DINH",
        "genre": "Insert a schema.org genre",
        "url": "https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/",
        "mainEntityOfPage": "https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/",
        "datePublished": "13-09-2020",
        "dateModified": "03-12-2020",
        "description": "This is my note for the 3rd course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..."
      }</script></article></main><footer><a href=/ target=_blank>Thi Â Â©Â  2020 </a>Â â€¢Â  <a href=/about-the-notes/ >About the notes </a>Â â€¢Â  <a href=https://pobo.dinhanhthi.com target=_blank>Po Bo </a>Â â€¢Â  <a href=/for-me-only/ >For me only </a>Â â€¢Â  <a href=/donate/ >Support Thi</a></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/elasticlunr/0.9.6/elasticlunr.min.js></script>