---
layout: post
title: "Accuracy metrics for model evaluation"
subtitle: Understanding some types of errors to evaluate models
description: "Understanding some types of errors to evaluate models"
tags: [machine learning, errors]
category: machine-learning
comment: 1
math: 1
---

Evaluation metrics are used to explain the performance of a model. Basically, we can compare the *actual values* and the *predicted values* to calculate the accuracy of our models. In this post, I try to understand the meaning and usage of some popular errors in the model evaluation.

{:.alert.alert-warning}
This article is not for you to learn, it's for refrence only!

{:.question}
## What's an error of the model?

The *error of the model* is the difference between the data points and the trend line generated by the algorithm. There are many ways to calculate this difference.

- **Mean Absolute Error** (MAE) : $MAE = \frac{1}{n}\sum\_{j=1}^n \vert y\_j - \hat{y}\_j \vert$.
- **Mean Squared Error** (MSE) : $MSE = \frac{1}{n}\sum\_{j=1}^n (y\_j - \hat{y}\_j)^2$
- **Root Mean Squared Error** (RMSR): $RMSR = \sqrt{\frac{1}{n}\sum\_{j=1}^n (y\_j - \hat{y}\_j)^2}$
- **Relative absolute Error** (RAE): $RAE = \dfrac{\sum\_{j=1}^n\vert y\_j-\hat{y}\_j\vert}{\sum\_{j=1}^n\vert y\_j-\bar{y}\vert}$ ($\bar{y}$ is the mean value of $y$)
- **Relative Squared Error** (RSE): $RSE = \dfrac{\sum\_{j=1}^n (y\_j-\hat{y}\_j)^2}{\sum\_{j=1}^n (y\_j-\bar{y}\_j)^2}$
- **R squared**: $R^2 = 1 - RSE$.

{:.question}
## Which one to choose?

- **MAE** : It's just the average error, the easiest one.
- **MSE** : It focuses on "larger" errors because of the squared term.
	- *MSE is more popular than MAE* because in the MAE, all gears are equivalent while in MSE, the bigger gears will influence much on the final error.
- **RMSE** : the most popular because it is interpretable *in the same units* as the response vector or y units.
- **RAE** : It's normalized, i.e. it doesn't depend much on the unit of $y$.
- **RSE** : It's used for calculating $R^2$.
- **$R^2$** : It represents how close the data values are, to the fitted regression line. The higher the R-squared, the better the model fits your data.

The choice of metric, completely depends on 

- The type of model, 
- Your data type
- Domain of knowledge.

{:.question}
## What's the train/test split problem?

It depends highly on the way we choose the train/test set data. That's why we need to use **cross-validation** evaluation to fix it. For example, K-fold cross validation.

{:.question}
## What's the idea of K-fold cross validation?

