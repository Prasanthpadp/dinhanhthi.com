---
layout: post
title: "Accuracy metrics for model evaluation"
subtitle: Understanding some types of errors to evaluate models
description: "Understanding some types of errors to evaluate models"
tags: [machine learning, errors]
category: machine-learning
comment: 1
math: 1
date: 2019-04-21
---

{% assign img-url = '/img/post/ML' %}

Evaluation metrics are used to explain the performance of a model. Basically, we can compare the *actual values* and the *predicted values* to calculate the accuracy of our models. In this post, I try to understand the meaning and usage of some popular errors in the model evaluation.

{:.alert.alert-warning}
This article is not for you to learn, it's for refrence only!

{:.question}
## What's an error of the model?

The *error of the model* is the difference between the data points and the trend line generated by the algorithm. There are many ways to calculate this difference (regression metrics)

- **Mean Absolute Error** (MAE) : $MAE = \frac{1}{n}\sum\_{j=1}^n \vert y\_j - \hat{y}\_j \vert$.
- **Mean Squared Error** (MSE) : $MSE = \frac{1}{n}\sum\_{j=1}^n (y\_j - \hat{y}\_j)^2$
- **Root Mean Squared Error** (RMSR): $RMSR = \sqrt{\frac{1}{n}\sum\_{j=1}^n (y\_j - \hat{y}\_j)^2}$
- **Relative absolute Error** (RAE): $RAE = \dfrac{\sum\_{j=1}^n\vert y\_j-\hat{y}\_j\vert}{\sum\_{j=1}^n\vert y\_j-\bar{y}\vert}$ ($\bar{y}$ is the mean value of $y$)
- **Relative Squared Error** (RSE): $RSE = \dfrac{\sum\_{j=1}^n (y\_j-\hat{y}\_j)^2}{\sum\_{j=1}^n (y\_j-\bar{y}\_j)^2}$
- **R squared**: $R^2 = 1 - RSE$.

{:.question}
## What'is their meaning?

- **MAE** : It's just the average error, the easiest one. All individual differences have the same role, there is no one being more weighted than the others.
- **MSE** : It focuses on "larger" errors because of the squared term. The higher this value, the worse the model is.
	- *MSE is more popular than MAE* because in the MAE, all gears are equivalent while in MSE, the bigger gears will influence much on the final error.
- **RMSE** : the most popular because it is interpretable *in the same units* as the response vector or $y$ units.
- **RAE** : It's normalized, i.e. it doesn't depend much on the unit of $y$.
- **RSE** : It's used for calculating $R^2$.
- **$R^2$** : It represents how close the data values are, to the fitted regression line. The higher the R-squared, the better the model fits your data.

The choice of metric, completely depends on 

- The type of model, 
- Your data type
- Domain of knowledge.

{:.question}
## What's the train/test split's problem?

It depends highly on the way we choose the train/test set data. That's why we need to use **cross-validation** evaluation to fix it. For example, K-fold cross validation.

~~~ python
# Split arrays or matrices into random train and test subsets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
~~~

If **we have enough data**, it may be a good choice.

{:.question}
## The idea of K-fold cross validation?

- We use CV to estimate how well a ML model would generalize to new data? It helps avoid [overfitting and underfitting]({{site.url}}{{site.baseurl}}/what-is-machine-learning#overfitting-underfitting).
- CV set and training set must use the same distribution!
- We choose different groups of CV set/training set to find the predictions, after that, we choose the best one. [Source of the figure](https://towardsdatascience.com/cross-validation-70289113a072).

	{:.img-full-normal}
	![K-fold Cross Validation idea]({{img-url}}/k-fold.png)

~~~ python
from sklearn.model_selection import KFold
kf = KFold(n_splits=2)
kf.get_n_splits(X)
for train_index, test_index in kf.split(X):
	X_train, X_test = X[train_index], X[test_index]
	y_train, y_test = y[train_index], y[test_index]
~~~